Quickstart Guide
This tutorial gives you a quick walkthrough about building an end-to-end language model application with LangChain.

Installation
To get started, install LangChain with the following command:

pip install langchain
# or
conda install langchain -c conda-forge
Environment Setup
Using LangChain will usually require integrations with one or more model providers, data stores, apis, etc.

For this example, we will be using OpenAI’s APIs, so we will first need to install their SDK:

pip install openai
We will then need to set the environment variable in the terminal.

export OPENAI_API_KEY="..."
Alternatively, you could do this from inside the Jupyter notebook (or Python script):

import os
os.environ["OPENAI_API_KEY"] = "..."
Building a Language Model Application: LLMs
Now that we have installed LangChain and set up our environment, we can start building our language model application.

LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications.

LLMs: Get predictions from a language model
The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.

In order to do this, we first need to import the LLM wrapper.

from langchain.llms import OpenAI
We can then initialize the wrapper with any arguments. In this example, we probably want the outputs to be MORE random, so we’ll initialize it with a HIGH temperature.

llm = OpenAI(temperature=0.9)
We can now call it on some input!

text = "What would be a good company name for a company that makes colorful socks?"
print(llm(text))
Feetful of Fun
For more details on how to use LLMs within LangChain, see the LLM getting started guide.

Prompt Templates: Manage prompts for LLMs
Calling an LLM is a great first step, but it’s just the beginning. Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you are probably taking user input and constructing a prompt, and then sending that to the LLM.

For example, in the previous example, the text we passed in was hardcoded to ask for a name for a company that made colorful socks. In this imaginary service, what we would want to do is take only the user input describing what the company does, and then format the prompt with that information.

This is easy to do with LangChain!

First lets define the prompt template:

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["product"],
    template="What is a good name for a company that makes {product}?",
)
Let’s now see how this works! We can call the .format method to format it.

print(prompt.format(product="colorful socks"))
What is a good name for a company that makes colorful socks?
For more details, check out the getting started guide for prompts.

Chains: Combine LLMs and prompts in multi-step workflows
Up until now, we’ve worked with the PromptTemplate and LLM primitives by themselves. But of course, a real application is not just one primitive, but rather a combination of them.

A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.

The most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.

Extending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.

from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

llm = OpenAI(temperature=0.9)
prompt = PromptTemplate(
    input_variables=["product"],
    template="What is a good name for a company that makes {product}?",
)
We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
Now we can run that chain only specifying the product!

chain.run("colorful socks")
# -> '\n\nSocktastic!'
There we go! There’s the first chain - an LLM Chain. This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.

For more details, check out the getting started guide for chains.

Agents: Dynamically Call Chains Based on User Input
So far the chains we’ve looked at run in a predetermined order.

Agents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.

When used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.

In order to load agents, you should understand the following concepts:

Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.

LLM: The language model powering the agent.

Agent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see the documentation for custom agents (coming soon).

Agents: For a list of supported agents and their specifications, see here.

Tools: For a list of predefined tools and their specifications, see here.

For this example, you will also need to install the SerpAPI Python package.

pip install google-search-results
And set the appropriate environment variables.

import os
os.environ["SERPAPI_API_KEY"] = "..."
Now we can get started!

from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.llms import OpenAI

# First, let's load the language model we're going to use to control the agent.
llm = OpenAI(temperature=0)

# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.
tools = load_tools(["serpapi", "llm-math"], llm=llm)


# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# Now let's test it out!
agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")
> Entering new AgentExecutor chain...
 I need to find the temperature first, then use the calculator to raise it to the .023 power.
Action: Search
Action Input: "High temperature in SF yesterday"
Observation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 °F (at 1:56 pm) Minimum temperature yesterday: 49 °F (at 1:56 am) Average temperature ...
Thought: I now have the temperature, so I can use the calculator to raise it to the .023 power.
Action: Calculator
Action Input: 57^.023
Observation: Answer: 1.0974509573251117

Thought: I now know the final answer
Final Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.

> Finished chain.
Memory: Add State to Chains and Agents
So far, all the chains and agents we’ve gone through have been stateless. But often, you may want a chain or agent to have some concept of “memory” so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of “short-term memory”. On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of “long-term memory”. For more concrete ideas on the latter, see this awesome paper.

LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory.

By default, the ConversationChain has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed. Let’s take a look at using this chain (setting verbose=True so we can see the prompt).

from langchain import OpenAI, ConversationChain

llm = OpenAI(temperature=0)
conversation = ConversationChain(llm=llm, verbose=True)

output = conversation.predict(input="Hi there!")
print(output)
> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:

> Finished chain.
' Hello! How are you today?'
output = conversation.predict(input="I'm doing well! Just having a conversation with an AI.")
print(output)
> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:  Hello! How are you today?
Human: I'm doing well! Just having a conversation with an AI.
AI:

> Finished chain.
" That's great! What would you like to talk about?"
Building a Language Model Application: Chat Models
Similarly, you can use chat models instead of LLMs. Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a “text in, text out” API, they expose an interface where “chat messages” are the inputs and outputs.

Chat model APIs are fairly new, so we are still figuring out the correct abstractions.

Get Message Completions from a Chat Model
You can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, and ChatMessage – ChatMessage takes in an arbitrary role parameter. Most of the time, you’ll just be dealing with HumanMessage, AIMessage, and SystemMessage.

from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

chat = ChatOpenAI(temperature=0)
You can get completions by passing in a single message.

chat([HumanMessage(content="Translate this sentence from English to French. I love programming.")])
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
You can also pass in multiple messages for OpenAI’s gpt-3.5-turbo and gpt-4 models.

messages = [
    SystemMessage(content="You are a helpful assistant that translates English to French."),
    HumanMessage(content="I love programming.")
]
chat(messages)
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
You can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter:

batch_messages = [
    [
        SystemMessage(content="You are a helpful assistant that translates English to French."),
        HumanMessage(content="I love programming.")
    ],
    [
        SystemMessage(content="You are a helpful assistant that translates English to French."),
        HumanMessage(content="I love artificial intelligence.")
    ],
]
result = chat.generate(batch_messages)
result
# -> LLMResult(generations=[[ChatGeneration(text="J'aime programmer.", generation_info=None, message=AIMessage(content="J'aime programmer.", additional_kwargs={}))], [ChatGeneration(text="J'aime l'intelligence artificielle.", generation_info=None, message=AIMessage(content="J'aime l'intelligence artificielle.", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})
You can recover things like token usage from this LLMResult:

result.llm_output['token_usage']
# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}
Chat Prompt Templates
Similar to LLMs, you can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate’s format_prompt – this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.

For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

chat = ChatOpenAI(temperature=0)

template = "You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# get a chat completion from the formatted messages
chat(chat_prompt.format_prompt(input_language="English", output_language="French", text="I love programming.").to_messages())
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
Chains with Chat Models
The LLMChain discussed in the above section can be used with chat models as well:

from langchain.chat_models import ChatOpenAI
from langchain import LLMChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

chat = ChatOpenAI(temperature=0)

template = "You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

chain = LLMChain(llm=chat, prompt=chat_prompt)
chain.run(input_language="English", output_language="French", text="I love programming.")
# -> "J'aime programmer."
Agents with Chat Models
Agents can also be used with chat models, you can initialize one using AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION as the agent type.

from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.chat_models import ChatOpenAI
from langchain.llms import OpenAI

# First, let's load the language model we're going to use to control the agent.
chat = ChatOpenAI(temperature=0)

# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.
llm = OpenAI(temperature=0)
tools = load_tools(["serpapi", "llm-math"], llm=llm)


# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.
agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# Now let's test it out!
agent.run("Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?")

> Entering new AgentExecutor chain...
Thought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.
Action:
{
  "action": "Search",
  "action_input": "Olivia Wilde boyfriend"
}

Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.
Thought:I need to use a search engine to find Harry Styles' current age.
Action:
{
  "action": "Search",
  "action_input": "Harry Styles age"
}

Observation: 29 years
Thought:Now I need to calculate 29 raised to the 0.23 power.
Action:
{
  "action": "Calculator",
  "action_input": "29^0.23"
}

Observation: Answer: 2.169459462491557

Thought:I now know the final answer.
Final Answer: 2.169459462491557

> Finished chain.
'2.169459462491557'
Memory: Add State to Chains and Agents
You can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, we can keep them as their own unique memory object.

from langchain.prompts import (
    ChatPromptTemplate, 
    MessagesPlaceholder, 
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate
)
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."),
    MessagesPlaceholder(variable_name="history"),
    HumanMessagePromptTemplate.from_template("{input}")
])

llm = ChatOpenAI(temperature=0)
memory = ConversationBufferMemory(return_messages=True)
conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)

conversation.predict(input="Hi there!")
# -> 'Hello! How can I assist you today?'


conversation.predict(input="I'm doing well! Just having a conversation with an AI.")
# -> "That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?"

conversation.predict(input="Tell me about yourself.")
# -> "Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?"


Concepts
These are concepts and terminology commonly used when developing LLM applications. It contains reference to external papers or sources where the concept was first introduced, as well as to places in LangChain where the concept is used.

Chain of Thought
Chain of Thought (CoT) is a prompting technique used to encourage the model to generate a series of intermediate reasoning steps. A less formal way to induce this behavior is to include “Let’s think step-by-step” in the prompt.


Action Plan Generation
Action Plan Generation is a prompting technique that uses a language model to generate actions to take. The results of these actions can then be fed back into the language model to generate a subsequent action.


ReAct
ReAct is a prompting technique that combines Chain-of-Thought prompting with action plan generation. This induces the to model to think about what action to take, then take it.

Self-ask
Self-ask is a prompting method that builds on top of chain-of-thought prompting. In this method, the model explicitly asks itself follow-up questions, which are then answered by an external search engine.


Prompt Chaining
Prompt Chaining is combining multiple LLM calls, with the output of one-step being the input to the next.

Memetic Proxy
Memetic Proxy is encouraging the LLM to respond in a certain way framing the discussion in a context that the model knows of and that will result in that type of response. For example, as a conversation between a student and a teacher.


Self Consistency
Self Consistency is a decoding strategy that samples a diverse set of reasoning paths and then selects the most consistent answer. Is most effective when combined with Chain-of-thought prompting.

Inception
Inception is also called First Person Instruction. It is encouraging the model to think a certain way by including the start of the model’s response in the prompt.

MemPrompt
MemPrompt maintains a memory of errors and user feedback, and uses them to prevent repetition of mistakes.


APIs

Embeddings
Wrappers around embedding modules.

pydantic model langchain.embeddings.AlephAlphaAsymmetricSemanticEmbedding[source]
Wrapper for Aleph Alpha’s Asymmetric Embeddings AA provides you with an endpoint to embed a document and a query. The models were optimized to make the embeddings of documents and the query for a document as similar as possible. To learn more, check out: https://docs.aleph-alpha.com/docs/tasks/semantic_embed/

Example

from aleph_alpha import AlephAlphaAsymmetricSemanticEmbedding

embeddings = AlephAlphaSymmetricSemanticEmbedding()

document = "This is a content of the document"
query = "What is the content of the document?"

doc_result = embeddings.embed_documents([document])
query_result = embeddings.embed_query(query)
field aleph_alpha_api_key: Optional[str] = None
API key for Aleph Alpha API.

field compress_to_size: Optional[int] = 128
Should the returned embeddings come back as an original 5120-dim vector, or should it be compressed to 128-dim.

field contextual_control_threshold: Optional[int] = None
Attention control parameters only apply to those tokens that have explicitly been set in the request.

field control_log_additive: Optional[bool] = True
Apply controls on prompt items by adding the log(control_factor) to attention scores.

field hosting: Optional[str] = 'https://api.aleph-alpha.com'
Optional parameter that specifies which datacenters may process the request.

field model: Optional[str] = 'luminous-base'
Model name to use.

field normalize: Optional[bool] = True
Should returned embeddings be normalized

embed_documents(texts: List[str]) → List[List[float]][source]
Call out to Aleph Alpha’s asymmetric Document endpoint.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Call out to Aleph Alpha’s asymmetric, query embedding endpoint :param text: The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.AlephAlphaSymmetricSemanticEmbedding[source]
The symmetric version of the Aleph Alpha’s semantic embeddings.

The main difference is that here, both the documents and queries are embedded with a SemanticRepresentation.Symmetric .. rubric:: Example

embed_documents(texts: List[str]) → List[List[float]][source]
Call out to Aleph Alpha’s Document endpoint.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Call out to Aleph Alpha’s asymmetric, query embedding endpoint :param text: The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.CohereEmbeddings[source]
Wrapper around Cohere embedding models.

To use, you should have the cohere python package installed, and the environment variable COHERE_API_KEY set with your API key or pass it as a named parameter to the constructor.

Example

from langchain.embeddings import CohereEmbeddings
cohere = CohereEmbeddings(
    model="embed-english-light-v2.0", cohere_api_key="my-api-key"
)
field model: str = 'embed-english-v2.0'
Model name to use.

field truncate: Optional[str] = None
Truncate embeddings that are too long from start or end (“NONE”|”START”|”END”)

embed_documents(texts: List[str]) → List[List[float]][source]
Call out to Cohere’s embedding endpoint.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Call out to Cohere’s embedding endpoint.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.FakeEmbeddings[source]
embed_documents(texts: List[str]) → List[List[float]][source]
Embed search docs.

embed_query(text: str) → List[float][source]
Embed query text.

pydantic model langchain.embeddings.HuggingFaceEmbeddings[source]
Wrapper around sentence_transformers embedding models.

To use, you should have the sentence_transformers python package installed.

Example

from langchain.embeddings import HuggingFaceEmbeddings

model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {'device': 'cpu'}
hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)
field cache_folder: Optional[str] = None
Path to store models. Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.

field encode_kwargs: Dict[str, Any] [Optional]
Key word arguments to pass when calling the encode method of the model.

field model_kwargs: Dict[str, Any] [Optional]
Key word arguments to pass to the model.

field model_name: str = 'sentence-transformers/all-mpnet-base-v2'
Model name to use.

embed_documents(texts: List[str]) → List[List[float]][source]
Compute doc embeddings using a HuggingFace transformer model.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Compute query embeddings using a HuggingFace transformer model.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.HuggingFaceHubEmbeddings[source]
Wrapper around HuggingFaceHub embedding models.

To use, you should have the huggingface_hub python package installed, and the environment variable HUGGINGFACEHUB_API_TOKEN set with your API token, or pass it as a named parameter to the constructor.

Example

from langchain.embeddings import HuggingFaceHubEmbeddings
repo_id = "sentence-transformers/all-mpnet-base-v2"
hf = HuggingFaceHubEmbeddings(
    repo_id=repo_id,
    task="feature-extraction",
    huggingfacehub_api_token="my-api-key",
)
field model_kwargs: Optional[dict] = None
Key word arguments to pass to the model.

field repo_id: str = 'sentence-transformers/all-mpnet-base-v2'
Model name to use.

field task: Optional[str] = 'feature-extraction'
Task to call the model with.

embed_documents(texts: List[str]) → List[List[float]][source]
Call out to HuggingFaceHub’s embedding endpoint for embedding search docs.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Call out to HuggingFaceHub’s embedding endpoint for embedding query text.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.HuggingFaceInstructEmbeddings[source]
Wrapper around sentence_transformers embedding models.

To use, you should have the sentence_transformers and InstructorEmbedding python packages installed.

Example

from langchain.embeddings import HuggingFaceInstructEmbeddings

model_name = "hkunlp/instructor-large"
model_kwargs = {'device': 'cpu'}
hf = HuggingFaceInstructEmbeddings(
    model_name=model_name, model_kwargs=model_kwargs
)
field cache_folder: Optional[str] = None
Path to store models. Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.

field embed_instruction: str = 'Represent the document for retrieval: '
Instruction to use for embedding documents.

field model_kwargs: Dict[str, Any] [Optional]
Key word arguments to pass to the model.

field model_name: str = 'hkunlp/instructor-large'
Model name to use.

field query_instruction: str = 'Represent the question for retrieving supporting documents: '
Instruction to use for embedding query.

embed_documents(texts: List[str]) → List[List[float]][source]
Compute doc embeddings using a HuggingFace instruct model.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Compute query embeddings using a HuggingFace instruct model.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.LlamaCppEmbeddings[source]
Wrapper around llama.cpp embedding models.

To use, you should have the llama-cpp-python library installed, and provide the path to the Llama model as a named parameter to the constructor. Check out: abetlen/llama-cpp-python

Example

from langchain.embeddings import LlamaCppEmbeddings
llama = LlamaCppEmbeddings(model_path="/path/to/model.bin")
field f16_kv: bool = False
Use half-precision for key/value cache.

field logits_all: bool = False
Return logits for all tokens, not just the last token.

field n_batch: Optional[int] = 8
Number of tokens to process in parallel. Should be a number between 1 and n_ctx.

field n_ctx: int = 512
Token context window.

field n_gpu_layers: Optional[int] = None
Number of layers to be loaded into gpu memory. Default None.

field n_parts: int = -1
Number of parts to split the model into. If -1, the number of parts is automatically determined.

field n_threads: Optional[int] = None
Number of threads to use. If None, the number of threads is automatically determined.

field seed: int = -1
Seed. If -1, a random seed is used.

field use_mlock: bool = False
Force system to keep model in RAM.

field vocab_only: bool = False
Only load the vocabulary, no weights.

embed_documents(texts: List[str]) → List[List[float]][source]
Embed a list of documents using the Llama model.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Embed a query using the Llama model.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.OpenAIEmbeddings[source]
Wrapper around OpenAI embedding models.

To use, you should have the openai python package installed, and the environment variable OPENAI_API_KEY set with your API key or pass it as a named parameter to the constructor.

Example

from langchain.embeddings import OpenAIEmbeddings
openai = OpenAIEmbeddings(openai_api_key="my-api-key")
In order to use the library with Microsoft Azure endpoints, you need to set the OPENAI_API_TYPE, OPENAI_API_BASE, OPENAI_API_KEY and OPENAI_API_VERSION. The OPENAI_API_TYPE must be set to ‘azure’ and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the model parameter.

Example

import os
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_BASE"] = "https://<your-endpoint.openai.azure.com/"
os.environ["OPENAI_API_KEY"] = "your AzureOpenAI key"
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"

from langchain.embeddings.openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(
    deployment="your-embeddings-deployment-name",
    model="your-embeddings-model-name",
    api_base="https://your-endpoint.openai.azure.com/",
    api_type="azure",
)
text = "This is a test query."
query_result = embeddings.embed_query(text)
field chunk_size: int = 1000
Maximum number of texts to embed in each batch

field max_retries: int = 6
Maximum number of retries to make when generating.

field request_timeout: Optional[Union[float, Tuple[float, float]]] = None
Timeout in seconds for the OpenAPI request.

embed_documents(texts: List[str], chunk_size: Optional[int] = 0) → List[List[float]][source]
Call out to OpenAI’s embedding endpoint for embedding search docs.

Parameters
texts – The list of texts to embed.

chunk_size – The chunk size of embeddings. If None, will use the chunk size specified by the class.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Call out to OpenAI’s embedding endpoint for embedding query text.

Parameters
text – The text to embed.

Returns
Embedding for the text.

pydantic model langchain.embeddings.SagemakerEndpointEmbeddings[source]
Wrapper around custom Sagemaker Inference Endpoints.

To use, you must supply the endpoint name from your deployed Sagemaker model & the region where it is deployed.

To authenticate, the AWS client uses the following methods to automatically load credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html

If a specific credential profile should be used, you must pass the name of the profile from the ~/.aws/credentials file that is to be used.

Make sure the credentials / roles used have the required policies to access the Sagemaker endpoint. See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html

field content_handler: langchain.embeddings.sagemaker_endpoint.EmbeddingsContentHandler [Required]
The content handler class that provides an input and output transform functions to handle formats between LLM and the endpoint.

field credentials_profile_name: Optional[str] = None
The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which has either access keys or role information specified. If not specified, the default credential profile or, if on an EC2 instance, credentials from IMDS will be used. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html

field endpoint_kwargs: Optional[Dict] = None
Optional attributes passed to the invoke_endpoint function. See `boto3`_. docs for more info. .. _boto3: <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html>

field endpoint_name: str = ''
The name of the endpoint from the deployed Sagemaker model. Must be unique within an AWS Region.

field model_kwargs: Optional[Dict] = None
Key word arguments to pass to the model.

field region_name: str = ''
The aws region where the Sagemaker model is deployed, eg. us-west-2.

embed_documents(texts: List[str], chunk_size: int = 64) → List[List[float]][source]
Compute doc embeddings using a SageMaker Inference Endpoint.

Parameters
texts – The list of texts to embed.

chunk_size – The chunk size defines how many input texts will be grouped together as request. If None, will use the chunk size specified by the class.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Compute query embeddings using a SageMaker inference endpoint.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.SelfHostedEmbeddings[source]
Runs custom embedding models on self-hosted remote hardware.

Supported hardware includes auto-launched instances on AWS, GCP, Azure, and Lambda, as well as servers specified by IP address and SSH credentials (such as on-prem, or another cloud like Paperspace, Coreweave, etc.).

To use, you should have the runhouse python package installed.

Example using a model load function:
from langchain.embeddings import SelfHostedEmbeddings
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import runhouse as rh

gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
def get_pipeline():
    model_id = "facebook/bart-large"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)
embeddings = SelfHostedEmbeddings(
    model_load_fn=get_pipeline,
    hardware=gpu
    model_reqs=["./", "torch", "transformers"],
)
Example passing in a pipeline path:
from langchain.embeddings import SelfHostedHFEmbeddings
import runhouse as rh
from transformers import pipeline

gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
pipeline = pipeline(model="bert-base-uncased", task="feature-extraction")
rh.blob(pickle.dumps(pipeline),
    path="models/pipeline.pkl").save().to(gpu, path="models")
embeddings = SelfHostedHFEmbeddings.from_pipeline(
    pipeline="models/pipeline.pkl",
    hardware=gpu,
    model_reqs=["./", "torch", "transformers"],
)
Validators
raise_deprecation » all fields

set_verbose » verbose

field inference_fn: Callable = <function _embed_documents>
Inference function to extract the embeddings on the remote hardware.

field inference_kwargs: Any = None
Any kwargs to pass to the model’s inference function.

embed_documents(texts: List[str]) → List[List[float]][source]
Compute doc embeddings using a HuggingFace transformer model.

Parameters
texts – The list of texts to embed.s

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Compute query embeddings using a HuggingFace transformer model.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

pydantic model langchain.embeddings.SelfHostedHuggingFaceEmbeddings[source]
Runs sentence_transformers embedding models on self-hosted remote hardware.

Supported hardware includes auto-launched instances on AWS, GCP, Azure, and Lambda, as well as servers specified by IP address and SSH credentials (such as on-prem, or another cloud like Paperspace, Coreweave, etc.).

To use, you should have the runhouse python package installed.

Example

from langchain.embeddings import SelfHostedHuggingFaceEmbeddings
import runhouse as rh
model_name = "sentence-transformers/all-mpnet-base-v2"
gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
hf = SelfHostedHuggingFaceEmbeddings(model_name=model_name, hardware=gpu)
Validators
raise_deprecation » all fields

set_verbose » verbose

field hardware: Any = None
Remote hardware to send the inference function to.

field inference_fn: Callable = <function _embed_documents>
Inference function to extract the embeddings.

field load_fn_kwargs: Optional[dict] = None
Key word arguments to pass to the model load function.

field model_id: str = 'sentence-transformers/all-mpnet-base-v2'
Model name to use.

field model_load_fn: Callable = <function load_embedding_model>
Function to load the model remotely on the server.

field model_reqs: List[str] = ['./', 'sentence_transformers', 'torch']
Requirements to install on hardware to inference the model.

pydantic model langchain.embeddings.SelfHostedHuggingFaceInstructEmbeddings[source]
Runs InstructorEmbedding embedding models on self-hosted remote hardware.

Supported hardware includes auto-launched instances on AWS, GCP, Azure, and Lambda, as well as servers specified by IP address and SSH credentials (such as on-prem, or another cloud like Paperspace, Coreweave, etc.).

To use, you should have the runhouse python package installed.

Example

from langchain.embeddings import SelfHostedHuggingFaceInstructEmbeddings
import runhouse as rh
model_name = "hkunlp/instructor-large"
gpu = rh.cluster(name='rh-a10x', instance_type='A100:1')
hf = SelfHostedHuggingFaceInstructEmbeddings(
    model_name=model_name, hardware=gpu)
Validators
raise_deprecation » all fields

set_verbose » verbose

field embed_instruction: str = 'Represent the document for retrieval: '
Instruction to use for embedding documents.

field model_id: str = 'hkunlp/instructor-large'
Model name to use.

field model_reqs: List[str] = ['./', 'InstructorEmbedding', 'torch']
Requirements to install on hardware to inference the model.

field query_instruction: str = 'Represent the question for retrieving supporting documents: '
Instruction to use for embedding query.

embed_documents(texts: List[str]) → List[List[float]][source]
Compute doc embeddings using a HuggingFace instruct model.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Compute query embeddings using a HuggingFace instruct model.

Parameters
text – The text to embed.

Returns
Embeddings for the text.

langchain.embeddings.SentenceTransformerEmbeddings
alias of langchain.embeddings.huggingface.HuggingFaceEmbeddings

pydantic model langchain.embeddings.TensorflowHubEmbeddings[source]
Wrapper around tensorflow_hub embedding models.

To use, you should have the tensorflow_text python package installed.

Example

from langchain.embeddings import TensorflowHubEmbeddings
url = "https://tfhub.dev/google/universal-sentence-encoder-multilingual/3"
tf = TensorflowHubEmbeddings(model_url=url)
field model_url: str = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'
Model name to use.

embed_documents(texts: List[str]) → List[List[float]][source]
Compute doc embeddings using a TensorflowHub embedding model.

Parameters
texts – The list of texts to embed.

Returns
List of embeddings, one for each text.

embed_query(text: str) → List[float][source]
Compute query embeddings using a TensorflowHub embedding model.

Parameters
text – The text to embed.

Returns
Embeddings for the text.


Chat Models
pydantic model langchain.chat_models.AzureChatOpenAI[source]
Wrapper around Azure OpenAI Chat Completion API. To use this class you must have a deployed model on Azure OpenAI. Use deployment_name in the constructor to refer to the “Model deployment name” in the Azure portal.

In addition, you should have the openai python package installed, and the following environment variables set or passed in constructor in lower case: - OPENAI_API_TYPE (default: azure) - OPENAI_API_KEY - OPENAI_API_BASE - OPENAI_API_VERSION

For exmaple, if you have gpt-35-turbo deployed, with the deployment name 35-turbo-dev, the constructor should look like:

Be aware the API version may change.

Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class.

field deployment_name: str = ''
field openai_api_base: str = ''
field openai_api_key: str = ''
Base URL path for API requests, leave blank if not using a proxy or service emulator.

field openai_api_type: str = 'azure'
field openai_api_version: str = ''
field openai_organization: str = ''
pydantic model langchain.chat_models.ChatAnthropic[source]
Wrapper around Anthropic’s large language model.

To use, you should have the anthropic python package installed, and the environment variable ANTHROPIC_API_KEY set with your API key, or pass it as a named parameter to the constructor.

Example

pydantic model langchain.chat_models.ChatGooglePalm[source]
Wrapper around Google’s PaLM Chat API.

To use you must have the google.generativeai Python package installed and either:

The GOOGLE_API_KEY` environment varaible set with your API key, or

Pass your API key using the google_api_key kwarg to the ChatGoogle constructor.

Example

from langchain.chat_models import ChatGooglePalm
chat = ChatGooglePalm()
field google_api_key: Optional[str] = None
field model_name: str = 'models/chat-bison-001'
Model name to use.

field n: int = 1
Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.

field temperature: Optional[float] = None
Run inference with this temperature. Must by in the closed interval [0.0, 1.0].

field top_k: Optional[int] = None
Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.

field top_p: Optional[float] = None
Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].

pydantic model langchain.chat_models.ChatOpenAI[source]
Wrapper around OpenAI Chat large language models.

To use, you should have the openai python package installed, and the environment variable OPENAI_API_KEY set with your API key.

Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class.

Example

from langchain.chat_models import ChatOpenAI
openai = ChatOpenAI(model_name="gpt-3.5-turbo")
field max_retries: int = 6
Maximum number of retries to make when generating.

field max_tokens: Optional[int] = None
Maximum number of tokens to generate.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field model_name: str = 'gpt-3.5-turbo'
Model name to use.

field n: int = 1
Number of chat completions to generate for each prompt.

field openai_api_base: Optional[str] = None
field openai_api_key: Optional[str] = None
Base URL path for API requests, leave blank if not using a proxy or service emulator.

field openai_organization: Optional[str] = None
field request_timeout: Optional[Union[float, Tuple[float, float]]] = None
Timeout for requests to OpenAI completion API. Default is 600 seconds.

field streaming: bool = False
Whether to stream the results or not.

field temperature: float = 0.7
What sampling temperature to use.

completion_with_retry(**kwargs: Any) → Any[source]
Use tenacity to retry the completion call.

get_num_tokens(text: str) → int[source]
Calculate num tokens with tiktoken package.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int[source]
Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.

Official documentation: openai/openai-cookbook main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb

pydantic model langchain.chat_models.PromptLayerChatOpenAI[source]
Wrapper around OpenAI Chat large language models and PromptLayer.

To use, you should have the openai and promptlayer python package installed, and the environment variable OPENAI_API_KEY and PROMPTLAYER_API_KEY set with your openAI API key and promptlayer key respectively.

All parameters that can be passed to the OpenAI LLM can also be passed here. The PromptLayerChatOpenAI adds to optional :param pl_tags: List of strings to tag the request with. :param return_pl_id: If True, the PromptLayer request ID will be

returned in the generation_info field of the Generation object.

Example

from langchain.chat_models import PromptLayerChatOpenAI
openai = PromptLayerChatOpenAI(model_name="gpt-3.5-turbo")
field pl_tags: Optional[List[str]] = None
field return_pl_id: Optional[bool] = False

Docstore
Wrappers on top of docstores.

class langchain.docstore.InMemoryDocstore(_dict: Dict[str, langchain.schema.Document])[source]
Simple in memory docstore in the form of a dict.

add(texts: Dict[str, langchain.schema.Document]) → None[source]
Add texts to in memory dictionary.

search(search: str) → Union[str, langchain.schema.Document][source]
Search via direct lookup.

class langchain.docstore.Wikipedia[source]
Wrapper around wikipedia API.

search(search: str) → Union[str, langchain.schema.Document][source]
Try to search for wiki page.

If page exists, return the page summary, and a PageWithLookups object. If page does not exist, return similar entries.


LLMs
Wrappers on top of large language models APIs.

pydantic model langchain.llms.AI21[source]
Wrapper around AI21 large language models.

To use, you should have the environment variable AI21_API_KEY set with your API key.

Example

from langchain.llms import AI21
ai21 = AI21(model="j2-jumbo-instruct")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field base_url: Optional[str] = None
Base url to use, if None decides based on model name.

field countPenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True)
Penalizes repeated tokens according to count.

field frequencyPenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True)
Penalizes repeated tokens according to frequency.

field logitBias: Optional[Dict[str, float]] = None
Adjust the probability of specific tokens being generated.

field maxTokens: int = 256
The maximum number of tokens to generate in the completion.

field minTokens: int = 0
The minimum number of tokens to generate in the completion.

field model: str = 'j2-jumbo-instruct'
Model name to use.

field numResults: int = 1
How many completions to generate for each prompt.

field presencePenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True)
Penalizes repeated tokens.

field temperature: float = 0.7
What sampling temperature to use.

field topP: float = 1.0
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.AlephAlpha[source]
Wrapper around Aleph Alpha large language models.

To use, you should have the aleph_alpha_client python package installed, and the environment variable ALEPH_ALPHA_API_KEY set with your API key, or pass it as a named parameter to the constructor.

Parameters are explained more in depth here: Aleph-Alpha/aleph-alpha-client

Example

from langchain.llms import AlephAlpha
alpeh_alpha = AlephAlpha(aleph_alpha_api_key="my-api-key")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field aleph_alpha_api_key: Optional[str] = None
API key for Aleph Alpha API.

field best_of: Optional[int] = None
returns the one with the “best of” results (highest log probability per token)

field completion_bias_exclusion_first_token_only: bool = False
Only consider the first token for the completion_bias_exclusion.

field contextual_control_threshold: Optional[float] = None
If set to None, attention control parameters only apply to those tokens that have explicitly been set in the request. If set to a non-None value, control parameters are also applied to similar tokens.

field control_log_additive: Optional[bool] = True
True: apply control by adding the log(control_factor) to attention scores. False: (attention_scores - - attention_scores.min(-1)) * control_factor

field echo: bool = False
Echo the prompt in the completion.

field frequency_penalty: float = 0.0
Penalizes repeated tokens according to frequency.

field log_probs: Optional[int] = None
Number of top log probabilities to be returned for each generated token.

field logit_bias: Optional[Dict[int, float]] = None
The logit bias allows to influence the likelihood of generating tokens.

field maximum_tokens: int = 64
The maximum number of tokens to be generated.

field minimum_tokens: Optional[int] = 0
Generate at least this number of tokens.

field model: Optional[str] = 'luminous-base'
Model name to use.

field n: int = 1
How many completions to generate for each prompt.

field penalty_bias: Optional[str] = None
Penalty bias for the completion.

field penalty_exceptions: Optional[List[str]] = None
List of strings that may be generated without penalty, regardless of other penalty settings

field penalty_exceptions_include_stop_sequences: Optional[bool] = None
Should stop_sequences be included in penalty_exceptions.

field presence_penalty: float = 0.0
Penalizes repeated tokens.

field raw_completion: bool = False
Force the raw completion of the model to be returned.

field repetition_penalties_include_completion: bool = True
Flag deciding whether presence penalty or frequency penalty are updated from the completion.

field repetition_penalties_include_prompt: Optional[bool] = False
Flag deciding whether presence penalty or frequency penalty are updated from the prompt.

field stop_sequences: Optional[List[str]] = None
Stop sequences to use.

field temperature: float = 0.0
A non-negative float that tunes the degree of randomness in generation.

field tokens: Optional[bool] = False
return tokens of completion.

field top_k: int = 0
Number of most likely tokens to consider at each step.

field top_p: float = 0.0
Total probability mass of tokens to consider at each step.

field use_multiplicative_presence_penalty: Optional[bool] = False
Flag deciding whether presence penalty is applied multiplicatively (True) or additively (False).

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Anthropic[source]
Wrapper around Anthropic’s large language models.

To use, you should have the anthropic python package installed, and the environment variable ANTHROPIC_API_KEY set with your API key, or pass it as a named parameter to the constructor.

Example

Validators
raise_deprecation » all fields

raise_warning » all fields

set_verbose » verbose

validate_environment » all fields

field default_request_timeout: Optional[Union[float, Tuple[float, float]]] = None
Timeout for requests to Anthropic Completion API. Default is 600 seconds.

field max_tokens_to_sample: int = 256
Denotes the number of tokens to predict per generation.

field model: str = 'claude-v1'
Model name to use.

field streaming: bool = False
Whether to stream the results.

field temperature: Optional[float] = None
A non-negative float that tunes the degree of randomness in generation.

field top_k: Optional[int] = None
Number of most likely tokens to consider at each step.

field top_p: Optional[float] = None
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

stream(prompt: str, stop: Optional[List[str]] = None) → Generator[source]
Call Anthropic completion_stream and return the resulting generator.

BETA: this is a beta feature while we figure out the right abstraction. Once that happens, this interface could change.

Parameters
prompt – The prompt to pass into the model.

stop – Optional list of stop words to use when generating.

Returns
A generator representing the stream of tokens from Anthropic.

Example

prompt = "Write a poem about a stream."
prompt = f"\n\nHuman: {prompt}\n\nAssistant:"
generator = anthropic.stream(prompt)
for token in generator:
    yield token
classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Anyscale[source]
Wrapper around Anyscale Services. To use, you should have the environment variable ANYSCALE_SERVICE_URL, ANYSCALE_SERVICE_ROUTE and ANYSCALE_SERVICE_TOKEN set with your Anyscale Service, or pass it as a named parameter to the constructor.

Example

Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field model_kwargs: Optional[dict] = None
Key word arguments to pass to the model. Reserved for future use

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.AzureOpenAI[source]
Wrapper around Azure-specific OpenAI large language models.

To use, you should have the openai python package installed, and the environment variable OPENAI_API_KEY set with your API key.

Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class.

Example

from langchain.llms import AzureOpenAI
openai = AzureOpenAI(model_name="text-davinci-003")
Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field allowed_special: Union[Literal['all'], AbstractSet[str]] = {}
Set of special tokens that are allowed。

field batch_size: int = 20
Batch size to use when passing multiple documents to generate.

field best_of: int = 1
Generates best_of completions server-side and returns the “best”.

field deployment_name: str = ''
Deployment name to use.

field disallowed_special: Union[Literal['all'], Collection[str]] = 'all'
Set of special tokens that are not allowed。

field frequency_penalty: float = 0
Penalizes repeated tokens according to frequency.

field logit_bias: Optional[Dict[str, float]] [Optional]
Adjust the probability of specific tokens being generated.

field max_retries: int = 6
Maximum number of retries to make when generating.

field max_tokens: int = 256
The maximum number of tokens to generate in the completion. -1 returns as many tokens as possible given the prompt and the models maximal context size.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field model_name: str = 'text-davinci-003'
Model name to use.

field n: int = 1
How many completions to generate for each prompt.

field presence_penalty: float = 0
Penalizes repeated tokens.

field request_timeout: Optional[Union[float, Tuple[float, float]]] = None
Timeout for requests to OpenAI completion API. Default is 600 seconds.

field streaming: bool = False
Whether to stream the results or not.

field temperature: float = 0.7
What sampling temperature to use.

field top_p: float = 1
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

create_llm_result(choices: Any, prompts: List[str], token_usage: Dict[str, int]) → langchain.schema.LLMResult
Create the LLMResult from the choices and prompts.

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Calculate num tokens with tiktoken package.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

get_sub_prompts(params: Dict[str, Any], prompts: List[str], stop: Optional[List[str]] = None) → List[List[str]]
Get the sub prompts for llm call.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

max_tokens_for_prompt(prompt: str) → int
Calculate the maximum number of tokens possible to generate for a prompt.

Parameters
prompt – The prompt to pass into the model.

Returns
The maximum number of tokens to generate for a prompt.

Example

max_tokens = openai.max_token_for_prompt("Tell me a joke.")
modelname_to_contextsize(modelname: str) → int
Calculate the maximum number of tokens possible to generate for a model.

Parameters
modelname – The modelname we want to know the context size for.

Returns
The maximum context size

Example

max_tokens = openai.modelname_to_contextsize("text-davinci-003")
predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

prep_streaming_params(stop: Optional[List[str]] = None) → Dict[str, Any]
Prepare the params for streaming.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

stream(prompt: str, stop: Optional[List[str]] = None) → Generator
Call OpenAI with streaming flag and return the resulting generator.

BETA: this is a beta feature while we figure out the right abstraction. Once that happens, this interface could change.

Parameters
prompt – The prompts to pass into the model.

stop – Optional list of stop words to use when generating.

Returns
A generator representing the stream of tokens from OpenAI.

Example

generator = openai.stream("Tell me a joke.")
for token in generator:
    yield token
classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Banana[source]
Wrapper around Banana large language models.

To use, you should have the banana-dev python package installed, and the environment variable BANANA_API_KEY set with your API key.

Any parameters that are valid to be passed to the call can be passed in, even if not explicitly saved on this class.

Example

Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field model_key: str = ''
model endpoint to use

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.CerebriumAI[source]
Wrapper around CerebriumAI large language models.

To use, you should have the cerebrium python package installed, and the environment variable CEREBRIUMAI_API_KEY set with your API key.

Any parameters that are valid to be passed to the call can be passed in, even if not explicitly saved on this class.

Example

Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field endpoint_url: str = ''
model endpoint to use

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Cohere[source]
Wrapper around Cohere large language models.

To use, you should have the cohere python package installed, and the environment variable COHERE_API_KEY set with your API key, or pass it as a named parameter to the constructor.

Example

from langchain.llms import Cohere
cohere = Cohere(model="gptd-instruct-tft", cohere_api_key="my-api-key")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field frequency_penalty: float = 0.0
Penalizes repeated tokens according to frequency. Between 0 and 1.

field k: int = 0
Number of most likely tokens to consider at each step.

field max_tokens: int = 256
Denotes the number of tokens to predict per generation.

field model: Optional[str] = None
Model name to use.

field p: int = 1
Total probability mass of tokens to consider at each step.

field presence_penalty: float = 0.0
Penalizes repeated tokens. Between 0 and 1.

field temperature: float = 0.75
A non-negative float that tunes the degree of randomness in generation.

field truncate: Optional[str] = None
Specify how the client handles inputs longer than the maximum token length: Truncate from START, END or NONE

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.DeepInfra[source]
Wrapper around DeepInfra deployed models.

To use, you should have the requests python package installed, and the environment variable DEEPINFRA_API_TOKEN set with your API token, or pass it as a named parameter to the constructor.

Only supports text-generation and text2text-generation for now.

Example

from langchain.llms import DeepInfra
di = DeepInfra(model_id="google/flan-t5-xl",
                    deepinfra_api_token="my-api-key")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.FakeListLLM[source]
Fake LLM wrapper for testing purposes.

Validators
raise_deprecation » all fields

set_verbose » verbose

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.ForefrontAI[source]
Wrapper around ForefrontAI large language models.

To use, you should have the environment variable FOREFRONTAI_API_KEY set with your API key.

Example

from langchain.llms import ForefrontAI
forefrontai = ForefrontAI(endpoint_url="")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field base_url: Optional[str] = None
Base url to use, if None decides based on model name.

field endpoint_url: str = ''
Model name to use.

field length: int = 256
The maximum number of tokens to generate in the completion.

field repetition_penalty: int = 1
Penalizes repeated tokens according to frequency.

field temperature: float = 0.7
What sampling temperature to use.

field top_k: int = 40
The number of highest probability vocabulary tokens to keep for top-k-filtering.

field top_p: float = 1.0
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.GPT4All[source]
Wrapper around GPT4All language models.

To use, you should have the pygpt4all python package installed, the pre-trained model file, and the model’s config information.

Example

from langchain.llms import GPT4All
model = GPT4All(model="./models/gpt4all-model.bin", n_ctx=512, n_threads=8)

# Simplest invocation
response = model("Once upon a time, ")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field echo: Optional[bool] = False
Whether to echo the prompt.

field embedding: bool = False
Use embedding mode only.

field f16_kv: bool = False
Use half-precision for key/value cache.

field logits_all: bool = False
Return logits for all tokens, not just the last token.

field model: str [Required]
Path to the pre-trained GPT4All model file.

field n_batch: int = 1
Batch size for prompt processing.

field n_ctx: int = 512
Token context window.

field n_parts: int = -1
Number of parts to split the model into. If -1, the number of parts is automatically determined.

field n_predict: Optional[int] = 256
The maximum number of tokens to generate.

field n_threads: Optional[int] = 4
Number of threads to use.

field repeat_last_n: Optional[int] = 64
Last n tokens to penalize

field repeat_penalty: Optional[float] = 1.3
The penalty to apply to repeated tokens.

field seed: int = 0
Seed. If -1, a random seed is used.

field stop: Optional[List[str]] = []
A list of strings to stop generation when encountered.

field streaming: bool = False
Whether to stream the results or not.

field temp: Optional[float] = 0.8
The temperature to use for sampling.

field top_k: Optional[int] = 40
The top-k value to use for sampling.

field top_p: Optional[float] = 0.95
The top-p value to use for sampling.

field use_mlock: bool = False
Force system to keep model in RAM.

field verbose: bool [Optional]
Whether to print out response text.

field vocab_only: bool = False
Only load the vocabulary, no weights.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.GooglePalm[source]
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field max_output_tokens: Optional[int] = None
Maximum number of tokens to include in a candidate. Must be greater than zero. If unset, will default to 64.

field model_name: str = 'models/text-bison-001'
Model name to use.

field n: int = 1
Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.

field temperature: float = 0.7
Run inference with this temperature. Must by in the closed interval [0.0, 1.0].

field top_k: Optional[int] = None
Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.

field top_p: Optional[float] = None
Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.GooseAI[source]
Wrapper around OpenAI large language models.

To use, you should have the openai python package installed, and the environment variable GOOSEAI_API_KEY set with your API key.

Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class.

Example

Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field frequency_penalty: float = 0
Penalizes repeated tokens according to frequency.

field logit_bias: Optional[Dict[str, float]] [Optional]
Adjust the probability of specific tokens being generated.

field max_tokens: int = 256
The maximum number of tokens to generate in the completion. -1 returns as many tokens as possible given the prompt and the models maximal context size.

field min_tokens: int = 1
The minimum number of tokens to generate in the completion.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field model_name: str = 'gpt-neo-20b'
Model name to use

field n: int = 1
How many completions to generate for each prompt.

field presence_penalty: float = 0
Penalizes repeated tokens.

field temperature: float = 0.7
What sampling temperature to use

field top_p: float = 1
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.HuggingFaceEndpoint[source]
Wrapper around HuggingFaceHub Inference Endpoints.

To use, you should have the huggingface_hub python package installed, and the environment variable HUGGINGFACEHUB_API_TOKEN set with your API token, or pass it as a named parameter to the constructor.

Only supports text-generation and text2text-generation for now.

Example

from langchain.llms import HuggingFaceEndpoint
endpoint_url = (
    "https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud"
)
hf = HuggingFaceEndpoint(
    endpoint_url=endpoint_url,
    huggingfacehub_api_token="my-api-key"
)
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field endpoint_url: str = ''
Endpoint URL to use.

field model_kwargs: Optional[dict] = None
Key word arguments to pass to the model.

field task: Optional[str] = None
Task to call the model with. Should be a task that returns generated_text or summary_text.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.HuggingFaceHub[source]
Wrapper around HuggingFaceHub models.

To use, you should have the huggingface_hub python package installed, and the environment variable HUGGINGFACEHUB_API_TOKEN set with your API token, or pass it as a named parameter to the constructor.

Only supports text-generation, text2text-generation and summarization for now.

Example

from langchain.llms import HuggingFaceHub
hf = HuggingFaceHub(repo_id="gpt2", huggingfacehub_api_token="my-api-key")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field model_kwargs: Optional[dict] = None
Key word arguments to pass to the model.

field repo_id: str = 'gpt2'
Model name to use.

field task: Optional[str] = None
Task to call the model with. Should be a task that returns generated_text or summary_text.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.HuggingFacePipeline[source]
Wrapper around HuggingFace Pipeline API.

To use, you should have the transformers python package installed.

Only supports text-generation, text2text-generation and summarization for now.

Example using from_model_id:
from langchain.llms import HuggingFacePipeline
hf = HuggingFacePipeline.from_model_id(
    model_id="gpt2", task="text-generation"
)
Example passing pipeline in directly:
from langchain.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_id = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
pipe = pipeline(
    "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10
)
hf = HuggingFacePipeline(pipeline=pipe)
Validators
raise_deprecation » all fields

set_verbose » verbose

field model_id: str = 'gpt2'
Model name to use.

field model_kwargs: Optional[dict] = None
Key word arguments to pass to the model.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

classmethod from_model_id(model_id: str, task: str, device: int = - 1, model_kwargs: Optional[dict] = None, **kwargs: Any) → langchain.llms.base.LLM[source]
Construct the pipeline object from model_id and task.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.HuggingFaceTextGenInference[source]
HuggingFace text generation inference API.

This class is a wrapper around the HuggingFace text generation inference API. It is used to generate text from a given prompt.

Attributes: - max_new_tokens: The maximum number of tokens to generate. - top_k: The number of top-k tokens to consider when generating text. - top_p: The cumulative probability threshold for generating text. - typical_p: The typical probability threshold for generating text. - temperature: The temperature to use when generating text. - repetition_penalty: The repetition penalty to use when generating text. - stop_sequences: A list of stop sequences to use when generating text. - seed: The seed to use when generating text. - inference_server_url: The URL of the inference server to use. - timeout: The timeout value in seconds to use while connecting to inference server. - client: The client object used to communicate with the inference server.

Methods: - _call: Generates text based on a given prompt and stop sequences. - _llm_type: Returns the type of LLM.

Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.HumanInputLLM[source]
A LLM wrapper which returns user input as the response.

Validators
raise_deprecation » all fields

set_verbose » verbose

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.LlamaCpp[source]
Wrapper around the llama.cpp model.

To use, you should have the llama-cpp-python library installed, and provide the path to the Llama model as a named parameter to the constructor. Check out: abetlen/llama-cpp-python

Example

from langchain.llms import LlamaCppEmbeddings
llm = LlamaCppEmbeddings(model_path="/path/to/llama/model")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field echo: Optional[bool] = False
Whether to echo the prompt.

field f16_kv: bool = True
Use half-precision for key/value cache.

field last_n_tokens_size: Optional[int] = 64
The number of tokens to look back when applying the repeat_penalty.

field logits_all: bool = False
Return logits for all tokens, not just the last token.

field logprobs: Optional[int] = None
The number of logprobs to return. If None, no logprobs are returned.

field lora_base: Optional[str] = None
The path to the Llama LoRA base model.

field lora_path: Optional[str] = None
The path to the Llama LoRA. If None, no LoRa is loaded.

field max_tokens: Optional[int] = 256
The maximum number of tokens to generate.

field model_path: str [Required]
The path to the Llama model file.

field n_batch: Optional[int] = 8
Number of tokens to process in parallel. Should be a number between 1 and n_ctx.

field n_ctx: int = 512
Token context window.

field n_gpu_layers: Optional[int] = None
Number of layers to be loaded into gpu memory. Default None.

field n_parts: int = -1
Number of parts to split the model into. If -1, the number of parts is automatically determined.

field n_threads: Optional[int] = None
Number of threads to use. If None, the number of threads is automatically determined.

field repeat_penalty: Optional[float] = 1.1
The penalty to apply to repeated tokens.

field seed: int = -1
Seed. If -1, a random seed is used.

field stop: Optional[List[str]] = []
A list of strings to stop generation when encountered.

field streaming: bool = True
Whether to stream the results, token by token.

field suffix: Optional[str] = None
A suffix to append to the generated text. If None, no suffix is appended.

field temperature: Optional[float] = 0.8
The temperature to use for sampling.

field top_k: Optional[int] = 40
The top-k value to use for sampling.

field top_p: Optional[float] = 0.95
The top-p value to use for sampling.

field use_mlock: bool = False
Force system to keep model in RAM.

field use_mmap: Optional[bool] = True
Whether to keep the model loaded in RAM

field verbose: bool [Optional]
Whether to print out response text.

field vocab_only: bool = False
Only load the vocabulary, no weights.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

stream(prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[langchain.callbacks.manager.CallbackManagerForLLMRun] = None) → Generator[Dict, None, None][source]
Yields results objects as they are generated in real time.

BETA: this is a beta feature while we figure out the right abstraction: Once that happens, this interface could change.

It also calls the callback manager’s on_llm_new_token event with similar parameters to the OpenAI LLM class method of the same name.

Args:
prompt: The prompts to pass into the model. stop: Optional list of stop words to use when generating.

Returns:
A generator representing the stream of tokens being generated.

Yields:
A dictionary like objects containing a string token and metadata. See llama-cpp-python docs and below for more.

Example:
from langchain.llms import LlamaCpp
llm = LlamaCpp(
    model_path="/path/to/local/model.bin",
    temperature = 0.5
)
for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'",
        stop=["'","
“]):
result = chunk[“choices”][0] print(result[“text”], end=’’, flush=True)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Modal[source]
Wrapper around Modal large language models.

To use, you should have the modal-client python package installed.

Any parameters that are valid to be passed to the call can be passed in, even if not explicitly saved on this class.

Example

Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

field endpoint_url: str = ''
model endpoint to use

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.NLPCloud[source]
Wrapper around NLPCloud large language models.

To use, you should have the nlpcloud python package installed, and the environment variable NLPCLOUD_API_KEY set with your API key.

Example

from langchain.llms import NLPCloud
nlpcloud = NLPCloud(model="gpt-neox-20b")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field bad_words: List[str] = []
List of tokens not allowed to be generated.

field do_sample: bool = True
Whether to use sampling (True) or greedy decoding.

field early_stopping: bool = False
Whether to stop beam search at num_beams sentences.

field length_no_input: bool = True
Whether min_length and max_length should include the length of the input.

field length_penalty: float = 1.0
Exponential penalty to the length.

field max_length: int = 256
The maximum number of tokens to generate in the completion.

field min_length: int = 1
The minimum number of tokens to generate in the completion.

field model_name: str = 'finetuned-gpt-neox-20b'
Model name to use.

field num_beams: int = 1
Number of beams for beam search.

field num_return_sequences: int = 1
How many completions to generate for each prompt.

field remove_end_sequence: bool = True
Whether or not to remove the end sequence token.

field remove_input: bool = True
Remove input text from API response

field repetition_penalty: float = 1.0
Penalizes repeated tokens. 1.0 means no penalty.

field temperature: float = 0.7
What sampling temperature to use.

field top_k: int = 50
The number of highest probability tokens to keep for top-k filtering.

field top_p: int = 1
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.OpenAI[source]
Wrapper around OpenAI large language models.

To use, you should have the openai python package installed, and the environment variable OPENAI_API_KEY set with your API key.

Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class.

Example

from langchain.llms import OpenAI
openai = OpenAI(model_name="text-davinci-003")
Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field allowed_special: Union[Literal['all'], AbstractSet[str]] = {}
Set of special tokens that are allowed。

field batch_size: int = 20
Batch size to use when passing multiple documents to generate.

field best_of: int = 1
Generates best_of completions server-side and returns the “best”.

field disallowed_special: Union[Literal['all'], Collection[str]] = 'all'
Set of special tokens that are not allowed。

field frequency_penalty: float = 0
Penalizes repeated tokens according to frequency.

field logit_bias: Optional[Dict[str, float]] [Optional]
Adjust the probability of specific tokens being generated.

field max_retries: int = 6
Maximum number of retries to make when generating.

field max_tokens: int = 256
The maximum number of tokens to generate in the completion. -1 returns as many tokens as possible given the prompt and the models maximal context size.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field model_name: str = 'text-davinci-003'
Model name to use.

field n: int = 1
How many completions to generate for each prompt.

field presence_penalty: float = 0
Penalizes repeated tokens.

field request_timeout: Optional[Union[float, Tuple[float, float]]] = None
Timeout for requests to OpenAI completion API. Default is 600 seconds.

field streaming: bool = False
Whether to stream the results or not.

field temperature: float = 0.7
What sampling temperature to use.

field top_p: float = 1
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

create_llm_result(choices: Any, prompts: List[str], token_usage: Dict[str, int]) → langchain.schema.LLMResult
Create the LLMResult from the choices and prompts.

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Calculate num tokens with tiktoken package.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

get_sub_prompts(params: Dict[str, Any], prompts: List[str], stop: Optional[List[str]] = None) → List[List[str]]
Get the sub prompts for llm call.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

max_tokens_for_prompt(prompt: str) → int
Calculate the maximum number of tokens possible to generate for a prompt.

Parameters
prompt – The prompt to pass into the model.

Returns
The maximum number of tokens to generate for a prompt.

Example

max_tokens = openai.max_token_for_prompt("Tell me a joke.")
modelname_to_contextsize(modelname: str) → int
Calculate the maximum number of tokens possible to generate for a model.

Parameters
modelname – The modelname we want to know the context size for.

Returns
The maximum context size

Example

max_tokens = openai.modelname_to_contextsize("text-davinci-003")
predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

prep_streaming_params(stop: Optional[List[str]] = None) → Dict[str, Any]
Prepare the params for streaming.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

stream(prompt: str, stop: Optional[List[str]] = None) → Generator
Call OpenAI with streaming flag and return the resulting generator.

BETA: this is a beta feature while we figure out the right abstraction. Once that happens, this interface could change.

Parameters
prompt – The prompts to pass into the model.

stop – Optional list of stop words to use when generating.

Returns
A generator representing the stream of tokens from OpenAI.

Example

generator = openai.stream("Tell me a joke.")
for token in generator:
    yield token
classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.OpenAIChat[source]
Wrapper around OpenAI Chat large language models.

To use, you should have the openai python package installed, and the environment variable OPENAI_API_KEY set with your API key.

Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class.

Example

from langchain.llms import OpenAIChat
openaichat = OpenAIChat(model_name="gpt-3.5-turbo")
Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field allowed_special: Union[Literal['all'], AbstractSet[str]] = {}
Set of special tokens that are allowed。

field disallowed_special: Union[Literal['all'], Collection[str]] = 'all'
Set of special tokens that are not allowed。

field max_retries: int = 6
Maximum number of retries to make when generating.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field model_name: str = 'gpt-3.5-turbo'
Model name to use.

field prefix_messages: List [Optional]
Series of messages for Chat input.

field streaming: bool = False
Whether to stream the results or not.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int[source]
Calculate num tokens with tiktoken package.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Petals[source]
Wrapper around Petals Bloom models.

To use, you should have the petals python package installed, and the environment variable HUGGINGFACE_API_KEY set with your API key.

Any parameters that are valid to be passed to the call can be passed in, even if not explicitly saved on this class.

Example

Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field client: Any = None
The client to use for the API calls.

field do_sample: bool = True
Whether or not to use sampling; use greedy decoding otherwise.

field max_length: Optional[int] = None
The maximum length of the sequence to be generated.

field max_new_tokens: int = 256
The maximum number of new tokens to generate in the completion.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field model_name: str = 'bigscience/bloom-petals'
The model to use.

field temperature: float = 0.7
What sampling temperature to use

field tokenizer: Any = None
The tokenizer to use for the API calls.

field top_k: Optional[int] = None
The number of highest probability vocabulary tokens to keep for top-k-filtering.

field top_p: float = 0.9
The cumulative probability for top-p sampling.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.PipelineAI[source]
Wrapper around PipelineAI large language models.

To use, you should have the pipeline-ai python package installed, and the environment variable PIPELINE_API_KEY set with your API key.

Any parameters that are valid to be passed to the call can be passed in, even if not explicitly saved on this class.

Example

Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field pipeline_key: str = ''
The id or tag of the target pipeline

field pipeline_kwargs: Dict[str, Any] [Optional]
Holds any pipeline parameters valid for create call not explicitly specified.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.PredictionGuard[source]
Wrapper around Prediction Guard large language models. To use, you should have the predictionguard python package installed, and the environment variable PREDICTIONGUARD_TOKEN set with your access token, or pass it as a named parameter to the constructor. .. rubric:: Example

Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field max_tokens: int = 256
Denotes the number of tokens to predict per generation.

field name: Optional[str] = 'default-text-gen'
Proxy name to use.

field temperature: float = 0.75
A non-negative float that tunes the degree of randomness in generation.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.PromptLayerOpenAI[source]
Wrapper around OpenAI large language models.

To use, you should have the openai and promptlayer python package installed, and the environment variable OPENAI_API_KEY and PROMPTLAYER_API_KEY set with your openAI API key and promptlayer key respectively.

All parameters that can be passed to the OpenAI LLM can also be passed here. The PromptLayerOpenAI LLM adds two optional :param pl_tags: List of strings to tag the request with. :param return_pl_id: If True, the PromptLayer request ID will be

returned in the generation_info field of the Generation object.

Example

from langchain.llms import PromptLayerOpenAI
openai = PromptLayerOpenAI(model_name="text-davinci-003")
Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

create_llm_result(choices: Any, prompts: List[str], token_usage: Dict[str, int]) → langchain.schema.LLMResult
Create the LLMResult from the choices and prompts.

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Calculate num tokens with tiktoken package.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

get_sub_prompts(params: Dict[str, Any], prompts: List[str], stop: Optional[List[str]] = None) → List[List[str]]
Get the sub prompts for llm call.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

max_tokens_for_prompt(prompt: str) → int
Calculate the maximum number of tokens possible to generate for a prompt.

Parameters
prompt – The prompt to pass into the model.

Returns
The maximum number of tokens to generate for a prompt.

Example

max_tokens = openai.max_token_for_prompt("Tell me a joke.")
modelname_to_contextsize(modelname: str) → int
Calculate the maximum number of tokens possible to generate for a model.

Parameters
modelname – The modelname we want to know the context size for.

Returns
The maximum context size

Example

max_tokens = openai.modelname_to_contextsize("text-davinci-003")
predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

prep_streaming_params(stop: Optional[List[str]] = None) → Dict[str, Any]
Prepare the params for streaming.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

stream(prompt: str, stop: Optional[List[str]] = None) → Generator
Call OpenAI with streaming flag and return the resulting generator.

BETA: this is a beta feature while we figure out the right abstraction. Once that happens, this interface could change.

Parameters
prompt – The prompts to pass into the model.

stop – Optional list of stop words to use when generating.

Returns
A generator representing the stream of tokens from OpenAI.

Example

generator = openai.stream("Tell me a joke.")
for token in generator:
    yield token
classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.PromptLayerOpenAIChat[source]
Wrapper around OpenAI large language models.

To use, you should have the openai and promptlayer python package installed, and the environment variable OPENAI_API_KEY and PROMPTLAYER_API_KEY set with your openAI API key and promptlayer key respectively.

All parameters that can be passed to the OpenAIChat LLM can also be passed here. The PromptLayerOpenAIChat adds two optional :param pl_tags: List of strings to tag the request with. :param return_pl_id: If True, the PromptLayer request ID will be

returned in the generation_info field of the Generation object.

Example

from langchain.llms import PromptLayerOpenAIChat
openaichat = PromptLayerOpenAIChat(model_name="gpt-3.5-turbo")
Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field allowed_special: Union[Literal['all'], AbstractSet[str]] = {}
Set of special tokens that are allowed。

field disallowed_special: Union[Literal['all'], Collection[str]] = 'all'
Set of special tokens that are not allowed。

field max_retries: int = 6
Maximum number of retries to make when generating.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field model_name: str = 'gpt-3.5-turbo'
Model name to use.

field prefix_messages: List [Optional]
Series of messages for Chat input.

field streaming: bool = False
Whether to stream the results or not.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Calculate num tokens with tiktoken package.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.RWKV[source]
Wrapper around RWKV language models.

To use, you should have the rwkv python package installed, the pre-trained model file, and the model’s config information.

Example

from langchain.llms import RWKV
model = RWKV(model="./models/rwkv-3b-fp16.bin", strategy="cpu fp32")

# Simplest invocation
response = model("Once upon a time, ")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field CHUNK_LEN: int = 256
Batch size for prompt processing.

field max_tokens_per_generation: int = 256
Maximum number of tokens to generate.

field model: str [Required]
Path to the pre-trained RWKV model file.

field penalty_alpha_frequency: float = 0.4
Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim..

field penalty_alpha_presence: float = 0.4
Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics..

field rwkv_verbose: bool = True
Print debug information.

field strategy: str = 'cpu fp32'
Token context window.

field temperature: float = 1.0
The temperature to use for sampling.

field tokens_path: str [Required]
Path to the RWKV tokens file.

field top_p: float = 0.5
The top-p value to use for sampling.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Replicate[source]
Wrapper around Replicate models.

To use, you should have the replicate python package installed, and the environment variable REPLICATE_API_TOKEN set with your API token. You can find your token here: https://replicate.com/account

The model param is required, but any other model parameters can also be passed in with the format input={model_param: value, …}

Example

Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.SagemakerEndpoint[source]
Wrapper around custom Sagemaker Inference Endpoints.

To use, you must supply the endpoint name from your deployed Sagemaker model & the region where it is deployed.

To authenticate, the AWS client uses the following methods to automatically load credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html

If a specific credential profile should be used, you must pass the name of the profile from the ~/.aws/credentials file that is to be used.

Make sure the credentials / roles used have the required policies to access the Sagemaker endpoint. See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html

Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field content_handler: langchain.llms.sagemaker_endpoint.LLMContentHandler [Required]
The content handler class that provides an input and output transform functions to handle formats between LLM and the endpoint.

field credentials_profile_name: Optional[str] = None
The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which has either access keys or role information specified. If not specified, the default credential profile or, if on an EC2 instance, credentials from IMDS will be used. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html

field endpoint_kwargs: Optional[Dict] = None
Optional attributes passed to the invoke_endpoint function. See `boto3`_. docs for more info. .. _boto3: <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html>

field endpoint_name: str = ''
The name of the endpoint from the deployed Sagemaker model. Must be unique within an AWS Region.

field model_kwargs: Optional[Dict] = None
Key word arguments to pass to the model.

field region_name: str = ''
The aws region where the Sagemaker model is deployed, eg. us-west-2.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.SelfHostedHuggingFaceLLM[source]
Wrapper around HuggingFace Pipeline API to run on self-hosted remote hardware.

Supported hardware includes auto-launched instances on AWS, GCP, Azure, and Lambda, as well as servers specified by IP address and SSH credentials (such as on-prem, or another cloud like Paperspace, Coreweave, etc.).

To use, you should have the runhouse python package installed.

Only supports text-generation, text2text-generation and summarization for now.

Example using from_model_id:
from langchain.llms import SelfHostedHuggingFaceLLM
import runhouse as rh
gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
hf = SelfHostedHuggingFaceLLM(
    model_id="google/flan-t5-large", task="text2text-generation",
    hardware=gpu
)
Example passing fn that generates a pipeline (bc the pipeline is not serializable):
from langchain.llms import SelfHostedHuggingFaceLLM
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import runhouse as rh

def get_pipeline():
    model_id = "gpt2"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    pipe = pipeline(
        "text-generation", model=model, tokenizer=tokenizer
    )
    return pipe
hf = SelfHostedHuggingFaceLLM(
    model_load_fn=get_pipeline, model_id="gpt2", hardware=gpu)
Validators
raise_deprecation » all fields

set_verbose » verbose

field device: int = 0
Device to use for inference. -1 for CPU, 0 for GPU, 1 for second GPU, etc.

field hardware: Any = None
Remote hardware to send the inference function to.

field inference_fn: Callable = <function _generate_text>
Inference function to send to the remote hardware.

field load_fn_kwargs: Optional[dict] = None
Key word arguments to pass to the model load function.

field model_id: str = 'gpt2'
Hugging Face model_id to load the model.

field model_kwargs: Optional[dict] = None
Key word arguments to pass to the model.

field model_load_fn: Callable = <function _load_transformer>
Function to load the model remotely on the server.

field model_reqs: List[str] = ['./', 'transformers', 'torch']
Requirements to install on hardware to inference the model.

field task: str = 'text-generation'
Hugging Face task (“text-generation”, “text2text-generation” or “summarization”).

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

classmethod from_pipeline(pipeline: Any, hardware: Any, model_reqs: Optional[List[str]] = None, device: int = 0, **kwargs: Any) → langchain.llms.base.LLM
Init the SelfHostedPipeline from a pipeline object or string.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.SelfHostedPipeline[source]
Run model inference on self-hosted remote hardware.

Supported hardware includes auto-launched instances on AWS, GCP, Azure, and Lambda, as well as servers specified by IP address and SSH credentials (such as on-prem, or another cloud like Paperspace, Coreweave, etc.).

To use, you should have the runhouse python package installed.

Example for custom pipeline and inference functions:
from langchain.llms import SelfHostedPipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import runhouse as rh

def load_pipeline():
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    return pipeline(
        "text-generation", model=model, tokenizer=tokenizer,
        max_new_tokens=10
    )
def inference_fn(pipeline, prompt, stop = None):
    return pipeline(prompt)[0]["generated_text"]

gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
llm = SelfHostedPipeline(
    model_load_fn=load_pipeline,
    hardware=gpu,
    model_reqs=model_reqs, inference_fn=inference_fn
)
Example for <2GB model (can be serialized and sent directly to the server):
from langchain.llms import SelfHostedPipeline
import runhouse as rh
gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
my_model = ...
llm = SelfHostedPipeline.from_pipeline(
    pipeline=my_model,
    hardware=gpu,
    model_reqs=["./", "torch", "transformers"],
)
Example passing model path for larger models:
from langchain.llms import SelfHostedPipeline
import runhouse as rh
import pickle
from transformers import pipeline

generator = pipeline(model="gpt2")
rh.blob(pickle.dumps(generator), path="models/pipeline.pkl"
    ).save().to(gpu, path="models")
llm = SelfHostedPipeline.from_pipeline(
    pipeline="models/pipeline.pkl",
    hardware=gpu,
    model_reqs=["./", "torch", "transformers"],
)
Validators
raise_deprecation » all fields

set_verbose » verbose

field hardware: Any = None
Remote hardware to send the inference function to.

field inference_fn: Callable = <function _generate_text>
Inference function to send to the remote hardware.

field load_fn_kwargs: Optional[dict] = None
Key word arguments to pass to the model load function.

field model_load_fn: Callable [Required]
Function to load the model remotely on the server.

field model_reqs: List[str] = ['./', 'torch']
Requirements to install on hardware to inference the model.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

classmethod from_pipeline(pipeline: Any, hardware: Any, model_reqs: Optional[List[str]] = None, device: int = 0, **kwargs: Any) → langchain.llms.base.LLM[source]
Init the SelfHostedPipeline from a pipeline object or string.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.StochasticAI[source]
Wrapper around StochasticAI large language models.

To use, you should have the environment variable STOCHASTICAI_API_KEY set with your API key.

Example

from langchain.llms import StochasticAI
stochasticai = StochasticAI(api_url="")
Validators
build_extra » all fields

raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field api_url: str = ''
Model name to use.

field model_kwargs: Dict[str, Any] [Optional]
Holds any model parameters valid for create call not explicitly specified.

field verbose: bool [Optional]
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.

pydantic model langchain.llms.Writer[source]
Wrapper around Writer large language models.

To use, you should have the environment variable WRITER_API_KEY and WRITER_ORG_ID set with your API key and organization ID respectively.

Example

from langchain import Writer
writer = Writer(model_id="palmyra-base")
Validators
raise_deprecation » all fields

set_verbose » verbose

validate_environment » all fields

field base_url: Optional[str] = None
Base url to use, if None decides based on model name.

field best_of: Optional[int] = None
Generates this many completions server-side and returns the “best”.

field logprobs: bool = False
Whether to return log probabilities.

field max_tokens: Optional[int] = None
Maximum number of tokens to generate.

field min_tokens: Optional[int] = None
Minimum number of tokens to generate.

field model_id: str = 'palmyra-instruct'
Model name to use.

field n: Optional[int] = None
How many completions to generate.

field presence_penalty: Optional[float] = None
Penalizes repeated tokens regardless of frequency.

field repetition_penalty: Optional[float] = None
Penalizes repeated tokens according to frequency.

field stop: Optional[List[str]] = None
Sequences when completion generation will stop.

field temperature: Optional[float] = None
What sampling temperature to use.

field top_p: Optional[float] = None
Total probability mass of tokens to consider at each step.

field verbose: bool [Optional]
Whether to print out response text.

field writer_api_key: Optional[str] = None
Writer API key.

field writer_org_id: Optional[str] = None
Writer organization ID.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → str
Check Cache and run the LLM on the given prompt and input.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model
Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values

copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model
Duplicate a model, optionally choose which fields to include, exclude and change.

Parameters
include – fields to include in new model

exclude – fields to exclude from new model, as with values this takes precedence over include

update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data

deep – set to True to make a deep copy of the model

Returns
new model instance

dict(**kwargs: Any) → Dict
Return a dictionary of the LLM.

generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Run the LLM on the given prompt and input.

generate_prompt(prompts: List[langchain.schema.PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager]] = None) → langchain.schema.LLMResult
Take in a list of prompt values and return an LLMResult.

get_num_tokens(text: str) → int
Get the number of tokens present in the text.

get_num_tokens_from_messages(messages: List[langchain.schema.BaseMessage]) → int
Get the number of tokens in the message.

json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode
Generate a JSON representation of the model, include and exclude arguments as per dict().

encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().

predict(text: str, *, stop: Optional[Sequence[str]] = None) → str
Predict text from text.

predict_messages(messages: List[langchain.schema.BaseMessage], *, stop: Optional[Sequence[str]] = None) → langchain.schema.BaseMessage
Predict message from messages.

save(file_path: Union[pathlib.Path, str]) → None
Save the LLM.

Parameters
file_path – Path to file to save the LLM to.

Example: .. code-block:: python

llm.save(file_path=”path/llm.yaml”)

classmethod update_forward_refs(**localns: Any) → None
Try to update ForwardRefs on fields based on this Model, globalns and localns.


Getting Started
LangChain primary focuses on constructing indexes with the goal of using them as a Retriever. In order to best understand what this means, it’s worth highlighting what the base Retriever interface is. The BaseRetriever class in LangChain is as follows:

from abc import ABC, abstractmethod
from typing import List
from langchain.schema import Document

class BaseRetriever(ABC):
    @abstractmethod
    def get_relevant_documents(self, query: str) -> List[Document]:
        """Get texts relevant for a query.

        Args:
            query: string to find relevant texts for

        Returns:
            List of relevant documents
        """
It’s that simple! The get_relevant_documents method can be implemented however you see fit.

Of course, we also help construct what we think useful Retrievers are. The main type of Retriever that we focus on is a Vectorstore retriever. We will focus on that for the rest of this guide.

In order to understand what a vectorstore retriever is, it’s important to understand what a Vectorstore is. So let’s look at that.

By default, LangChain uses Chroma as the vectorstore to index and search embeddings. To walk through this tutorial, we’ll first need to install chromadb.

pip install chromadb
This example showcases question answering over documents. We have chosen this as the example for getting started because it nicely combines a lot of different elements (Text splitters, embeddings, vectorstores) and then also shows how to use them in a chain.

Question answering over documents consists of four steps:

Create an index

Create a Retriever from that index

Create a question answering chain

Ask questions!

Each of the steps has multiple sub steps and potential configurations. In this notebook we will primarily focus on (1). We will start by showing the one-liner for doing so, but then break down what is actually going on.

First, let’s import some common classes we’ll use no matter what.

from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
Next in the generic setup, let’s specify the document loader we want to use. You can download the state_of_the_union.txt file here

from langchain.document_loaders import TextLoader
loader = TextLoader('../state_of_the_union.txt', encoding='utf8')
One Line Index Creation
To get started as quickly as possible, we can use the VectorstoreIndexCreator.

from langchain.indexes import VectorstoreIndexCreator
index = VectorstoreIndexCreator().from_loaders([loader])
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transient.
Now that the index is created, we can use it to ask questions of the data! Note that under the hood this is actually doing a few steps as well, which we will cover later in this guide.

query = "What did the president say about Ketanji Brown Jackson"
index.query(query)
" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans."
query = "What did the president say about Ketanji Brown Jackson"
index.query_with_sources(query)
{'question': 'What did the president say about Ketanji Brown Jackson',
 'answer': " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, one of the nation's top legal minds, to continue Justice Breyer's legacy of excellence, and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\n",
 'sources': '../state_of_the_union.txt'}
What is returned from the VectorstoreIndexCreator is VectorStoreIndexWrapper, which provides these nice query and query_with_sources functionality. If we just wanted to access the vectorstore directly, we can also do that.

index.vectorstore
<langchain.vectorstores.chroma.Chroma at 0x119aa5940>
If we then want to access the VectorstoreRetriever, we can do that with:

index.vectorstore.as_retriever()
VectorStoreRetriever(vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x119aa5940>, search_kwargs={})
Walkthrough
Okay, so what’s actually going on? How is this index getting created?

A lot of the magic is being hid in this VectorstoreIndexCreator. What is this doing?

There are three main steps going on after the documents are loaded:

Splitting documents into chunks

Creating embeddings for each document

Storing documents and embeddings in a vectorstore

Let’s walk through this in code

documents = loader.load()
Next, we will split the documents into chunks.

from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
We will then select which embeddings we want to use.

from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()
We now create the vectorstore to use as the index.

from langchain.vectorstores import Chroma
db = Chroma.from_documents(texts, embeddings)
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transient.
So that’s creating the index. Then, we expose this index in a retriever interface.

retriever = db.as_retriever()
Then, as before, we create a chain and use it to answer questions!

qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=retriever)
query = "What did the president say about Ketanji Brown Jackson"
qa.run(query)
" The President said that Judge Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He said she is a consensus builder and has received a broad range of support from organizations such as the Fraternal Order of Police and former judges appointed by Democrats and Republicans."
VectorstoreIndexCreator is just a wrapper around all this logic. It is configurable in the text splitter it uses, the embeddings it uses, and the vectorstore it uses. For example, you can configure it as below:

index_creator = VectorstoreIndexCreator(
    vectorstore_cls=Chroma, 
    embedding=OpenAIEmbeddings(),
    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
)
Hopefully this highlights what is going on under the hood of VectorstoreIndexCreator. While we think it’s important to have a simple way to create indexes, we also think it’s important to understand what’s going on under the hood.


Getting Started
The default recommended text splitter is the RecursiveCharacterTextSplitter. This text splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character, and so forth. By default the characters it tries to split on are ["\n\n", "\n", " ", ""]

In addition to controlling which characters you can split on, you can also control a few other things:

length_function: how the length of chunks is calculated. Defaults to just counting number of characters, but it’s pretty common to pass a token counter here.

chunk_size: the maximum size of your chunks (as measured by the length function).

chunk_overlap: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (eg do a sliding window).

# This is a long document we can split up.
with open('../../state_of_the_union.txt') as f:
    state_of_the_union = f.read()
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
)
texts = text_splitter.create_documents([state_of_the_union])
print(texts[0])
print(texts[1])
page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' lookup_str='' metadata={} lookup_index=0
page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' lookup_str='' metadata={} lookup_index=0

tiktoken (OpenAI) Length Function
You can also use tiktoken, an open source tokenizer package from OpenAI to estimate tokens used. Will probably be more accurate for their models.

How the text is split: by character passed in

How the chunk size is measured: by tiktoken tokenizer

# This is a long document we can split up.
with open('../../../state_of_the_union.txt') as f:
    state_of_the_union = f.read()
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(state_of_the_union)
print(texts[0])


Getting Started
This notebook showcases basic functionality related to VectorStores. A key part of working with vectorstores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the embedding notebook before diving into this.

This covers generic high level functionality related to all vector stores.

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
with open('../../state_of_the_union.txt') as f:
    state_of_the_union = f.read()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_text(state_of_the_union)

embeddings = OpenAIEmbeddings()
docsearch = Chroma.from_texts(texts, embeddings)

query = "What did the president say about Ketanji Brown Jackson"
docs = docsearch.similarity_search(query)
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transient.
print(docs[0].page_content)
In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. 

We cannot let this happen. 

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
Add texts
You can easily add text to a vectorstore with the add_texts method. It will return a list of document IDs (in case you need to use them downstream).

docsearch.add_texts(["Ankush went to Princeton"])
['a05e3d0c-ab40-11ed-a853-e65801318981']
query = "Where did Ankush go to college?"
docs = docsearch.similarity_search(query)
docs[0]
Document(page_content='Ankush went to Princeton', lookup_str='', metadata={}, lookup_index=0)
From Documents
We can also initialize a vectorstore from documents directly. This is useful when we use the method on the text splitter to get documents directly (handy when the original documents have associated metadata).

documents = text_splitter.create_documents([state_of_the_union], metadatas=[{"source": "State of the Union"}])
docsearch = Chroma.from_documents(documents, embeddings)

query = "What did the president say about Ketanji Brown Jackson"
docs = docsearch.similarity_search(query)
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transient.
print(docs[0].page_content)


Chroma
Chroma is a database for building AI applications with embeddings.

This notebook shows how to use functionality related to the Chroma vector database.

!pip install chromadb
# get a token: https://platform.openai.com/account/api-keys

from getpass import getpass

OPENAI_API_KEY = getpass()
import os

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain.document_loaders import TextLoader
loader = TextLoader('../../../state_of_the_union.txt')
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(docs, embeddings)

query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)
Using embedded DuckDB without persistence: data will be transient
print(docs[0].page_content)
Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
Similarity search with score
docs = db.similarity_search_with_score(query)
docs[0]
(Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),
 0.3949805498123169)
Persistance
The below steps cover how to persist a ChromaDB instance

Initialize PeristedChromaDB
Create embeddings for each chunk and insert into the Chroma vector database. The persist_directory argument tells ChromaDB where to store the database when it’s persisted.

# Embed and store the texts
# Supplying a persist_directory will store the embeddings on disk
persist_directory = 'db'

embedding = OpenAIEmbeddings()
vectordb = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=persist_directory)
Running Chroma using direct local API.
No existing DB found in db, skipping load
No existing DB found in db, skipping load
Persist the Database
We should call persist() to ensure the embeddings are written to disk.

vectordb.persist()
vectordb = None
Persisting DB to disk, putting it in the save folder db
PersistentDuckDB del, about to run persist
Persisting DB to disk, putting it in the save folder db
Load the Database from disk, and create the chain
Be sure to pass the same persist_directory and embedding_function as you did when you instantiated the database. Initialize the chain we will use for question answering.

# Now we can load the persisted database from disk, and use it as normal. 
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
Running Chroma using direct local API.
loaded in 4 embeddings
loaded in 1 collections
Retriever options
This section goes over different options for how to use Chroma as a retriever.

MMR
In addition to using similarity search in the retriever object, you can also use mmr.

retriever = db.as_retriever(search_type="mmr")
retriever.get_relevant_documents(query)[0]
Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})



SupabaseVectorStore
Supabase is an open source Firebase alternative.

This notebook shows how to use Supabase and pgvector as your VectorStore.

To run this notebook, please ensure:

the pgvector extension is enabled

you have installed the supabase-py package

that you have created a match_documents function in your database

that you have a documents table in your public schema similar to the one below.

The following function determines cosine similarity, but you can adjust to your needs.

       -- Enable the pgvector extension to work with embedding vectors
       create extension vector;

       -- Create a table to store your documents
       create table documents (
       id bigserial primary key,
       content text, -- corresponds to Document.pageContent
       metadata jsonb, -- corresponds to Document.metadata
       embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
       );

       CREATE FUNCTION match_documents(query_embedding vector(1536), match_count int)
           RETURNS TABLE(
               id bigint,
               content text,
               metadata jsonb,
               -- we return matched vectors to enable maximal marginal relevance searches
               embedding vector(1536),
               similarity float)
           LANGUAGE plpgsql
           AS $$
           # variable_conflict use_column
       BEGIN
           RETURN query
           SELECT
               id,
               content,
               metadata,
               embedding,
               1 -(documents.embedding <=> query_embedding) AS similarity
           FROM
               documents
           ORDER BY
               documents.embedding <=> query_embedding
           LIMIT match_count;
       END;
       $$;
# with pip
!pip install supabase

# with conda
# !conda install -c conda-forge supabase
We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.

import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
os.environ['SUPABASE_URL'] = getpass.getpass('Supabase URL:')
os.environ['SUPABASE_SERVICE_KEY'] = getpass.getpass('Supabase Service Key:')
# If you're storing your Supabase and OpenAI API keys in a .env file, you can load them with dotenv
from dotenv import load_dotenv

load_dotenv()
True
import os
from supabase.client import Client, create_client

supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_SERVICE_KEY")
supabase: Client = create_client(supabase_url, supabase_key)
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import SupabaseVectorStore
from langchain.document_loaders import TextLoader
2023-04-19 20:12:28,593:INFO - NumExpr defaulting to 8 threads.
from langchain.document_loaders import TextLoader

loader = TextLoader("../../../state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
# We're using the default `documents` table here. You can modify this by passing in a `table_name` argument to the `from_documents` method.
vector_store = SupabaseVectorStore.from_documents(
    docs, embeddings, client=supabase
)
query = "What did the president say about Ketanji Brown Jackson"
matched_docs = vector_store.similarity_search(query)
print(matched_docs[0].page_content)
Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
Similarity search with score
matched_docs = vector_store.similarity_search_with_relevance_scores(query)
matched_docs[0]
(Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),
 0.802509746274066)
Retriever options
This section goes over different options for how to use SupabaseVectorStore as a retriever.

Maximal Marginal Relevance Searches
In addition to using similarity search in the retriever object, you can also use mmr.

retriever = vector_store.as_retriever(search_type="mmr")
matched_docs = retriever.get_relevant_documents(query)
for i, d in enumerate(matched_docs):
    print(f"\n## Document {i}\n")
    print(d.page_content)



    The retriever interface is a generic interface that makes it easy to combine documents with language models. This interface exposes a get_relevant_documents method which takes in a query (a string) and returns a list of documents.

Please see below for a list of all the retrievers supported.



Getting Started
This notebook walks through how LangChain thinks about memory.

Memory involves keeping a concept of state around throughout a user’s interactions with an language model. A user’s interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.

In general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain.

Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.

In this notebook, we will walk through the simplest form of memory: “buffer” memory, which just involves keeping a buffer of all prior messages. We will show how to use the modular utility functions here, then show how it can be used in a chain (both returning a string as well as a list of messages).

ChatMessageHistory
One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper which exposes convenience methods for saving Human messages, AI messages, and then fetching them all.

You may want to use this class directly if you are managing memory outside of a chain.

from langchain.memory import ChatMessageHistory

history = ChatMessageHistory()

history.add_user_message("hi!")

history.add_ai_message("whats up?")
history.messages
[HumanMessage(content='hi!', additional_kwargs={}),
 AIMessage(content='whats up?', additional_kwargs={})]
ConversationBufferMemory
We now show how to use this simple concept in a chain. We first showcase ConversationBufferMemory which is just a wrapper around ChatMessageHistory that extracts the messages in a variable.

We can first extract it as a string.

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory()
memory.chat_memory.add_user_message("hi!")
memory.chat_memory.add_ai_message("whats up?")
memory.load_memory_variables({})
{'history': 'Human: hi!\nAI: whats up?'}
We can also get the history as a list of messages

memory = ConversationBufferMemory(return_messages=True)
memory.chat_memory.add_user_message("hi!")
memory.chat_memory.add_ai_message("whats up?")
memory.load_memory_variables({})
{'history': [HumanMessage(content='hi!', additional_kwargs={}),
  AIMessage(content='whats up?', additional_kwargs={})]}
Using in a chain
Finally, let’s take a look at using this in a chain (setting verbose=True so we can see the prompt).

from langchain.llms import OpenAI
from langchain.chains import ConversationChain


llm = OpenAI(temperature=0)
conversation = ConversationChain(
    llm=llm, 
    verbose=True, 
    memory=ConversationBufferMemory()
)
conversation.predict(input="Hi there!")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:

> Finished chain.
" Hi there! It's nice to meet you. How can I help you today?"
conversation.predict(input="I'm doing well! Just having a conversation with an AI.")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi there!
AI:  Hi there! It's nice to meet you. How can I help you today?
Human: I'm doing well! Just having a conversation with an AI.
AI:

> Finished chain.
" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?"
conversation.predict(input="Tell me about yourself.")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi there!
AI:  Hi there! It's nice to meet you. How can I help you today?
Human: I'm doing well! Just having a conversation with an AI.
AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?
Human: Tell me about yourself.
AI:

> Finished chain.
" Sure! I'm an AI created to help people with their everyday tasks. I'm programmed to understand natural language and provide helpful information. I'm also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers."
Saving Message History
You may often have to save messages, and then load them to use again. This can be done easily by first converting the messages to normal python dictionaries, saving those (as json or something) and then loading those. Here is an example of doing that.

import json

from langchain.memory import ChatMessageHistory
from langchain.schema import messages_from_dict, messages_to_dict

history = ChatMessageHistory()

history.add_user_message("hi!")

history.add_ai_message("whats up?")
dicts = messages_to_dict(history.messages)
dicts
[{'type': 'human', 'data': {'content': 'hi!', 'additional_kwargs': {}}},
 {'type': 'ai', 'data': {'content': 'whats up?', 'additional_kwargs': {}}}]
new_messages = messages_from_dict(dicts)
new_messages
[HumanMessage(content='hi!', additional_kwargs={}),
 AIMessage(content='whats up?', additional_kwargs={})]


 ConversationBufferMemory
This notebook shows how to use ConversationBufferMemory. This memory allows for storing of messages and then extracts the messages in a variable.

We can first extract it as a string.

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory()
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.load_memory_variables({})
{'history': 'Human: hi\nAI: whats up'}
We can also get the history as a list of messages (this is useful if you are using this with a chat model).

memory = ConversationBufferMemory(return_messages=True)
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.load_memory_variables({})
{'history': [HumanMessage(content='hi', additional_kwargs={}),
  AIMessage(content='whats up', additional_kwargs={})]}
Using in a chain
Finally, let’s take a look at using this in a chain (setting verbose=True so we can see the prompt).

from langchain.llms import OpenAI
from langchain.chains import ConversationChain


llm = OpenAI(temperature=0)
conversation = ConversationChain(
    llm=llm, 
    verbose=True, 
    memory=ConversationBufferMemory()
)
conversation.predict(input="Hi there!")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:

> Finished chain.
" Hi there! It's nice to meet you. How can I help you today?"
conversation.predict(input="I'm doing well! Just having a conversation with an AI.")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi there!
AI:  Hi there! It's nice to meet you. How can I help you today?
Human: I'm doing well! Just having a conversation with an AI.
AI:

> Finished chain.
" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?"
conversation.predict(input="Tell me about yourself.")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi there!
AI:  Hi there! It's nice to meet you. How can I help you today?
Human: I'm doing well! Just having a conversation with an AI.
AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?
Human: Tell me about yourself.
AI:

> Finished chain.
" Sure! I'm an AI created to help people with their everyday tasks. I'm programmed to understand natural language and provide helpful information. I'm also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers."



VectorStore-Backed Memory
VectorStoreRetrieverMemory stores memories in a VectorDB and queries the top-K most “salient” docs every time it is called.

This differs from most of the other Memory classes in that it doesn’t explicitly track the order of interactions.

In this case, the “docs” are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation.

from datetime import datetime
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.memory import VectorStoreRetrieverMemory
from langchain.chains import ConversationChain
from langchain.prompts import PromptTemplate
Initialize your VectorStore
Depending on the store you choose, this step may look different. Consult the relevant VectorStore documentation for more details.

import faiss

from langchain.docstore import InMemoryDocstore
from langchain.vectorstores import FAISS


embedding_size = 1536 # Dimensions of the OpenAIEmbeddings
index = faiss.IndexFlatL2(embedding_size)
embedding_fn = OpenAIEmbeddings().embed_query
vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})
Create your the VectorStoreRetrieverMemory
The memory object is instantiated from any VectorStoreRetriever.

# In actual usage, you would set `k` to be a higher value, but we use k=1 to show that
# the vector lookup still returns the semantically relevant information
retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))
memory = VectorStoreRetrieverMemory(retriever=retriever)

# When added to an agent, the memory object can save pertinent information from conversations or used tools
memory.save_context({"input": "My favorite food is pizza"}, {"output": "thats good to know"})
memory.save_context({"input": "My favorite sport is soccer"}, {"output": "..."})
memory.save_context({"input": "I don't the Celtics"}, {"output": "ok"}) # 
# Notice the first result returned is the memory pertaining to tax help, which the language model deems more semantically relevant
# to a 1099 than the other documents, despite them both containing numbers.
print(memory.load_memory_variables({"prompt": "what sport should i watch?"})["history"])
input: My favorite sport is soccer
output: ...
Using in a chain
Let’s walk through an example, again setting verbose=True so we can see the prompt.

llm = OpenAI(temperature=0) # Can be any valid LLM
_DEFAULT_TEMPLATE = """The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
{history}

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: {input}
AI:"""
PROMPT = PromptTemplate(
    input_variables=["history", "input"], template=_DEFAULT_TEMPLATE
)
conversation_with_summary = ConversationChain(
    llm=llm, 
    prompt=PROMPT,
    # We set a very low max_token_limit for the purposes of testing.
    memory=memory,
    verbose=True
)
conversation_with_summary.predict(input="Hi, my name is Perry, what's up?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
input: My favorite food is pizza
output: thats good to know

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: Hi, my name is Perry, what's up?
AI:

> Finished chain.
" Hi Perry, I'm doing well. How about you?"
# Here, the basketball related content is surfaced
conversation_with_summary.predict(input="what's my favorite sport?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
input: My favorite sport is soccer
output: ...

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: what's my favorite sport?
AI:

> Finished chain.
' You told me earlier that your favorite sport is soccer.'
# Even though the language model is stateless, since relavent memory is fetched, it can "reason" about the time.
# Timestamping memories and data is useful in general to let the agent determine temporal relevance
conversation_with_summary.predict(input="Whats my favorite food")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
input: My favorite food is pizza
output: thats good to know

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: Whats my favorite food
AI:

> Finished chain.
' You said your favorite food is pizza.'
# The memories from the conversation are automatically stored,
# since this query best matches the introduction chat above,
# the agent is able to 'remember' the user's name.
conversation_with_summary.predict(input="What's my name?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
input: Hi, my name is Perry, what's up?
response:  Hi Perry, I'm doing well. How about you?

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: What's my name?
AI:

> Finished chain.
' Your name is Perry.'



ConversationBufferWindowMemory
ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large

Let’s first explore the basic functionality of this type of memory.

from langchain.memory import ConversationBufferWindowMemory
memory = ConversationBufferWindowMemory( k=1)
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.save_context({"input": "not much you"}, {"ouput": "not much"})
memory.load_memory_variables({})
{'history': 'Human: not much you\nAI: not much'}
We can also get the history as a list of messages (this is useful if you are using this with a chat model).

memory = ConversationBufferWindowMemory( k=1, return_messages=True)
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.save_context({"input": "not much you"}, {"ouput": "not much"})
memory.load_memory_variables({})
{'history': [HumanMessage(content='not much you', additional_kwargs={}),
  AIMessage(content='not much', additional_kwargs={})]}
Using in a chain
Let’s walk through an example, again setting verbose=True so we can see the prompt.

from langchain.llms import OpenAI
from langchain.chains import ConversationChain
conversation_with_summary = ConversationChain(
    llm=OpenAI(temperature=0), 
    # We set a low k=2, to only keep the last 2 interactions in memory
    memory=ConversationBufferWindowMemory(k=2), 
    verbose=True
)
conversation_with_summary.predict(input="Hi, what's up?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi, what's up?
AI:

> Finished chain.
" Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?"
conversation_with_summary.predict(input="What's their issues?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi, what's up?
AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?
Human: What's their issues?
AI:

> Finished chain.
" The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected."
conversation_with_summary.predict(input="Is it going well?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi, what's up?
AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?
Human: What's their issues?
AI:  The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected.
Human: Is it going well?
AI:

> Finished chain.
" Yes, it's going well so far. We've already identified the problem and are now working on a solution."
# Notice here that the first interaction does not appear.
conversation_with_summary.predict(input="What's the solution?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: What's their issues?
AI:  The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected.
Human: Is it going well?
AI:  Yes, it's going well so far. We've already identified the problem and are now working on a solution.
Human: What's the solution?
AI:

> Finished chain.
" The solution is to reset the router and reconfigure the settings. We're currently in the process of doing that."



Entity Memory
This notebook shows how to work with a memory module that remembers things about specific entities. It extracts information on entities (using LLMs) and builds up its knowledge about that entity over time (also using LLMs).

Let’s first walk through using this functionality.

from langchain.llms import OpenAI
from langchain.memory import ConversationEntityMemory
llm = OpenAI(temperature=0)
memory = ConversationEntityMemory(llm=llm)
_input = {"input": "Deven & Sam are working on a hackathon project"}
memory.load_memory_variables(_input)
memory.save_context(
    _input,
    {"ouput": " That sounds like a great project! What kind of project are they working on?"}
)
memory.load_memory_variables({"input": 'who is Sam'})
{'history': 'Human: Deven & Sam are working on a hackathon project\nAI:  That sounds like a great project! What kind of project are they working on?',
 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}}
memory = ConversationEntityMemory(llm=llm, return_messages=True)
_input = {"input": "Deven & Sam are working on a hackathon project"}
memory.load_memory_variables(_input)
memory.save_context(
    _input,
    {"ouput": " That sounds like a great project! What kind of project are they working on?"}
)
memory.load_memory_variables({"input": 'who is Sam'})
{'history': [HumanMessage(content='Deven & Sam are working on a hackathon project', additional_kwargs={}),
  AIMessage(content=' That sounds like a great project! What kind of project are they working on?', additional_kwargs={})],
 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}}
Using in a chain
Let’s now use it in a chain!

from langchain.chains import ConversationChain
from langchain.memory import ConversationEntityMemory
from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE
from pydantic import BaseModel
from typing import List, Dict, Any
conversation = ConversationChain(
    llm=llm, 
    verbose=True,
    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,
    memory=ConversationEntityMemory(llm=llm)
)
conversation.predict(input="Deven & Sam are working on a hackathon project")
> Entering new ConversationChain chain...
Prompt after formatting:
You are an assistant to a human, powered by a large language model trained by OpenAI.

You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.

Context:
{'Deven': 'Deven is working on a hackathon project with Sam.', 'Sam': 'Sam is working on a hackathon project with Deven.'}

Current conversation:

Last line:
Human: Deven & Sam are working on a hackathon project
You:

> Finished chain.
' That sounds like a great project! What kind of project are they working on?'
conversation.memory.entity_store.store
{'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon.',
 'Sam': 'Sam is working on a hackathon project with Deven.'}
conversation.predict(input="They are trying to add more complex memory structures to Langchain")
> Entering new ConversationChain chain...
Prompt after formatting:
You are an assistant to a human, powered by a large language model trained by OpenAI.

You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.

Context:
{'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon.', 'Sam': 'Sam is working on a hackathon project with Deven.', 'Langchain': ''}

Current conversation:
Human: Deven & Sam are working on a hackathon project
AI:  That sounds like a great project! What kind of project are they working on?
Last line:
Human: They are trying to add more complex memory structures to Langchain
You:

> Finished chain.
' That sounds like an interesting project! What kind of memory structures are they trying to add?'
conversation.predict(input="They are adding in a key-value store for entities mentioned so far in the conversation.")
> Entering new ConversationChain chain...
Prompt after formatting:
You are an assistant to a human, powered by a large language model trained by OpenAI.

You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.

Context:
{'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain.', 'Langchain': 'Langchain is a project that is trying to add more complex memory structures.', 'Key-Value Store': ''}

Current conversation:
Human: Deven & Sam are working on a hackathon project
AI:  That sounds like a great project! What kind of project are they working on?
Human: They are trying to add more complex memory structures to Langchain
AI:  That sounds like an interesting project! What kind of memory structures are they trying to add?
Last line:
Human: They are adding in a key-value store for entities mentioned so far in the conversation.
You:

> Finished chain.
' That sounds like a great idea! How will the key-value store help with the project?'
conversation.predict(input="What do you know about Deven & Sam?")
> Entering new ConversationChain chain...
Prompt after formatting:
You are an assistant to a human, powered by a large language model trained by OpenAI.

You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.

Context:
{'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation.'}

Current conversation:
Human: Deven & Sam are working on a hackathon project
AI:  That sounds like a great project! What kind of project are they working on?
Human: They are trying to add more complex memory structures to Langchain
AI:  That sounds like an interesting project! What kind of memory structures are they trying to add?
Human: They are adding in a key-value store for entities mentioned so far in the conversation.
AI:  That sounds like a great idea! How will the key-value store help with the project?
Last line:
Human: What do you know about Deven & Sam?
You:

> Finished chain.
' Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help.'
Inspecting the memory store
We can also inspect the memory store directly. In the following examaples, we look at it directly, and then go through some examples of adding information and watch how it changes.

from pprint import pprint
pprint(conversation.memory.entity_store.store)
{'Daimon': 'Daimon is a company founded by Sam, a successful entrepreneur.',
 'Deven': 'Deven is working on a hackathon project with Sam, which they are '
          'entering into a hackathon. They are trying to add more complex '
          'memory structures to Langchain, including a key-value store for '
          'entities mentioned so far in the conversation, and seem to be '
          'working hard on this project with a great idea for how the '
          'key-value store can help.',
 'Key-Value Store': 'A key-value store is being added to the project to store '
                    'entities mentioned in the conversation.',
 'Langchain': 'Langchain is a project that is trying to add more complex '
              'memory structures, including a key-value store for entities '
              'mentioned so far in the conversation.',
 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more '
        'complex memory structures to Langchain, including a key-value store '
        'for entities mentioned so far in the conversation. They seem to have '
        'a great idea for how the key-value store can help, and Sam is also '
        'the founder of a company called Daimon.'}
conversation.predict(input="Sam is the founder of a company called Daimon.")
> Entering new ConversationChain chain...
Prompt after formatting:
You are an assistant to a human, powered by a large language model trained by OpenAI.

You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.

Context:
{'Daimon': 'Daimon is a company founded by Sam, a successful entrepreneur.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to have a great idea for how the key-value store can help, and Sam is also the founder of a company called Daimon.'}

Current conversation:
Human: They are adding in a key-value store for entities mentioned so far in the conversation.
AI:  That sounds like a great idea! How will the key-value store help with the project?
Human: What do you know about Deven & Sam?
AI:  Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help.
Human: Sam is the founder of a company called Daimon.
AI: 
That's impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon?
Last line:
Human: Sam is the founder of a company called Daimon.
You:


Conversation Knowledge Graph Memory
This type of memory uses a knowledge graph to recreate memory.

Let’s first walk through how to use the utilities

from langchain.memory import ConversationKGMemory
from langchain.llms import OpenAI
llm = OpenAI(temperature=0)
memory = ConversationKGMemory(llm=llm)
memory.save_context({"input": "say hi to sam"}, {"ouput": "who is sam"})
memory.save_context({"input": "sam is a friend"}, {"ouput": "okay"})
memory.load_memory_variables({"input": 'who is sam'})
{'history': 'On Sam: Sam is friend.'}
We can also get the history as a list of messages (this is useful if you are using this with a chat model).

memory = ConversationKGMemory(llm=llm, return_messages=True)
memory.save_context({"input": "say hi to sam"}, {"ouput": "who is sam"})
memory.save_context({"input": "sam is a friend"}, {"ouput": "okay"})
memory.load_memory_variables({"input": 'who is sam'})
{'history': [SystemMessage(content='On Sam: Sam is friend.', additional_kwargs={})]}
We can also more modularly get current entities from a new message (will use previous messages as context.)

memory.get_current_entities("what's Sams favorite color?")
['Sam']
We can also more modularly get knowledge triplets from a new message (will use previous messages as context.)

memory.get_knowledge_triplets("her favorite color is red")
[KnowledgeTriple(subject='Sam', predicate='favorite color', object_='red')]
Using in a chain
Let’s now use this in a chain!

llm = OpenAI(temperature=0)
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import ConversationChain

template = """The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. 
If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the "Relevant Information" section and does not hallucinate.

Relevant Information:

{history}

Conversation:
Human: {input}
AI:"""
prompt = PromptTemplate(
    input_variables=["history", "input"], template=template
)
conversation_with_kg = ConversationChain(
    llm=llm, 
    verbose=True, 
    prompt=prompt,
    memory=ConversationKGMemory(llm=llm)
)
conversation_with_kg.predict(input="Hi, what's up?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. 
If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the "Relevant Information" section and does not hallucinate.

Relevant Information:



Conversation:
Human: Hi, what's up?
AI:

> Finished chain.
" Hi there! I'm doing great. I'm currently in the process of learning about the world around me. I'm learning about different cultures, languages, and customs. It's really fascinating! How about you?"
conversation_with_kg.predict(input="My name is James and I'm helping Will. He's an engineer.")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. 
If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the "Relevant Information" section and does not hallucinate.

Relevant Information:



Conversation:
Human: My name is James and I'm helping Will. He's an engineer.
AI:

> Finished chain.
" Hi James, it's nice to meet you. I'm an AI and I understand you're helping Will, the engineer. What kind of engineering does he do?"
conversation_with_kg.predict(input="What do you know about Will?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. 
If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the "Relevant Information" section and does not hallucinate.

Relevant Information:

On Will: Will is an engineer.

Conversation:
Human: What do you know about Will?
AI:

> Finished chain.
' Will is an engineer.'



ConversationSummaryMemory
Now let’s take a look at using a slightly more complex type of memory - ConversationSummaryMemory. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.

Let’s first explore the basic functionality of this type of memory.

from langchain.memory import ConversationSummaryMemory, ChatMessageHistory
from langchain.llms import OpenAI
memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.load_memory_variables({})
{'history': '\nThe human greets the AI, to which the AI responds.'}
We can also get the history as a list of messages (this is useful if you are using this with a chat model).

memory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True)
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.load_memory_variables({})
{'history': [SystemMessage(content='\nThe human greets the AI, to which the AI responds.', additional_kwargs={})]}
We can also utilize the predict_new_summary method directly.

messages = memory.chat_memory.messages
previous_summary = ""
memory.predict_new_summary(messages, previous_summary)
'\nThe human greets the AI, to which the AI responds.'
Initializing with messages
If you have messages outside this class, you can easily initialize the class with ChatMessageHistory. During loading, a summary will be calculated.

history = ChatMessageHistory()
history.add_user_message("hi")
history.add_ai_message("hi there!")
memory = ConversationSummaryMemory.from_messages(llm=OpenAI(temperature=0), chat_memory=history, return_messages=True)
memory.buffer
'\nThe human greets the AI, to which the AI responds with a friendly greeting.'
Using in a chain
Let’s walk through an example of using this in a chain, again setting verbose=True so we can see the prompt.

from langchain.llms import OpenAI
from langchain.chains import ConversationChain
llm = OpenAI(temperature=0)
conversation_with_summary = ConversationChain(
    llm=llm, 
    memory=ConversationSummaryMemory(llm=OpenAI()),
    verbose=True
)
conversation_with_summary.predict(input="Hi, what's up?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi, what's up?
AI:

> Finished chain.
" Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?"
conversation_with_summary.predict(input="Tell me more about it!")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue.
Human: Tell me more about it!
AI:

> Finished chain.
" Sure! The customer is having trouble with their computer not connecting to the internet. I'm helping them troubleshoot the issue and figure out what the problem is. So far, we've tried resetting the router and checking the network settings, but the issue still persists. We're currently looking into other possible solutions."
conversation_with_summary.predict(input="Very cool -- what is the scope of the project?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue where their computer was not connecting to the internet. The AI was troubleshooting the issue and had already tried resetting the router and checking the network settings, but the issue still persisted and they were looking into other possible solutions.
Human: Very cool -- what is the scope of the project?
AI:

> Finished chain.
" The scope of the project is to troubleshoot the customer's computer issue and find a solution that will allow them to connect to the internet. We are currently exploring different possibilities and have already tried resetting the router and checking the network settings, but the issue still persists."



ConversationSummaryBufferMemory
ConversationSummaryBufferMemory combines the last two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. Unlike the previous implementation though, it uses token length rather than number of interactions to determine when to flush interactions.

Let’s first walk through how to use the utilities

from langchain.memory import ConversationSummaryBufferMemory
from langchain.llms import OpenAI
llm = OpenAI()
memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.save_context({"input": "not much you"}, {"output": "not much"})
memory.load_memory_variables({})
{'history': 'System: \nThe human says "hi", and the AI responds with "whats up".\nHuman: not much you\nAI: not much'}
We can also get the history as a list of messages (this is useful if you are using this with a chat model).

memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10, return_messages=True)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.save_context({"input": "not much you"}, {"output": "not much"})
We can also utilize the predict_new_summary method directly.

messages = memory.chat_memory.messages
previous_summary = ""
memory.predict_new_summary(messages, previous_summary)
'\nThe human and AI state that they are not doing much.'
Using in a chain
Let’s walk through an example, again setting verbose=True so we can see the prompt.

from langchain.chains import ConversationChain
conversation_with_summary = ConversationChain(
    llm=llm, 
    # We set a very low max_token_limit for the purposes of testing.
    memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40),
    verbose=True
)
conversation_with_summary.predict(input="Hi, what's up?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi, what's up?
AI:

> Finished chain.
" Hi there! I'm doing great. I'm learning about the latest advances in artificial intelligence. What about you?"
conversation_with_summary.predict(input="Just working on writing some documentation!")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi, what's up?
AI:  Hi there! I'm doing great. I'm spending some time learning about the latest developments in AI technology. How about you?
Human: Just working on writing some documentation!
AI:

> Finished chain.
' That sounds like a great use of your time. Do you have experience with writing documentation?'
# We can see here that there is a summary of the conversation and then some previous interactions
conversation_with_summary.predict(input="For LangChain! Have you heard of it?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
System: 
The human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology.
Human: Just working on writing some documentation!
AI:  That sounds like a great use of your time. Do you have experience with writing documentation?
Human: For LangChain! Have you heard of it?
AI:

> Finished chain.
" No, I haven't heard of LangChain. Can you tell me more about it?"
# We can see here that the summary and the buffer are updated
conversation_with_summary.predict(input="Haha nope, although a lot of people confuse it for that")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
System: 
The human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology. The human then mentioned they were writing documentation, to which the AI responded that it sounded like a great use of their time and asked if they had experience with writing documentation.
Human: For LangChain! Have you heard of it?
AI:  No, I haven't heard of LangChain. Can you tell me more about it?
Human: Haha nope, although a lot of people confuse it for that
AI:

> Finished chain.
' Oh, okay. What is LangChain?'





ConversationTokenBufferMemory
ConversationTokenBufferMemory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.

Let’s first walk through how to use the utilities

from langchain.memory import ConversationTokenBufferMemory
from langchain.llms import OpenAI
llm = OpenAI()
memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.save_context({"input": "not much you"}, {"ouput": "not much"})
memory.load_memory_variables({})
{'history': 'Human: not much you\nAI: not much'}
We can also get the history as a list of messages (this is useful if you are using this with a chat model).

memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10, return_messages=True)
memory.save_context({"input": "hi"}, {"ouput": "whats up"})
memory.save_context({"input": "not much you"}, {"ouput": "not much"})
Using in a chain
Let’s walk through an example, again setting verbose=True so we can see the prompt.

from langchain.chains import ConversationChain
conversation_with_summary = ConversationChain(
    llm=llm, 
    # We set a very low max_token_limit for the purposes of testing.
    memory=ConversationTokenBufferMemory(llm=OpenAI(), max_token_limit=60),
    verbose=True
)
conversation_with_summary.predict(input="Hi, what's up?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi, what's up?
AI:

> Finished chain.
" Hi there! I'm doing great, just enjoying the day. How about you?"
conversation_with_summary.predict(input="Just working on writing some documentation!")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi, what's up?
AI:  Hi there! I'm doing great, just enjoying the day. How about you?
Human: Just working on writing some documentation!
AI:

> Finished chain.
' Sounds like a productive day! What kind of documentation are you writing?'
conversation_with_summary.predict(input="For LangChain! Have you heard of it?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi, what's up?
AI:  Hi there! I'm doing great, just enjoying the day. How about you?
Human: Just working on writing some documentation!
AI:  Sounds like a productive day! What kind of documentation are you writing?
Human: For LangChain! Have you heard of it?
AI:

> Finished chain.
" Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you're writing about?"
# We can see here that the buffer is updated
conversation_with_summary.predict(input="Haha nope, although a lot of people confuse it for that")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: For LangChain! Have you heard of it?
AI:  Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you're writing about?
Human: Haha nope, although a lot of people confuse it for that
AI:

> Finished chain.
" Oh, I see. Is there another language learning platform you're referring to?"



How to add Memory to an LLMChain
This notebook goes over how to use the Memory class with an LLMChain. For the purposes of this walkthrough, we will add the ConversationBufferMemory class, although this can be any memory class.

from langchain.memory import ConversationBufferMemory
from langchain import OpenAI, LLMChain, PromptTemplate
The most important step is setting up the prompt correctly. In the below prompt, we have two input keys: one for the actual input, another for the input from the Memory class. Importantly, we make sure the keys in the PromptTemplate and the ConversationBufferMemory match up (chat_history).

template = """You are a chatbot having a conversation with a human.

{chat_history}
Human: {human_input}
Chatbot:"""

prompt = PromptTemplate(
    input_variables=["chat_history", "human_input"], 
    template=template
)
memory = ConversationBufferMemory(memory_key="chat_history")
llm_chain = LLMChain(
    llm=OpenAI(), 
    prompt=prompt, 
    verbose=True, 
    memory=memory,
)
llm_chain.predict(human_input="Hi there my friend")
> Entering new LLMChain chain...
Prompt after formatting:
You are a chatbot having a conversation with a human.


Human: Hi there my friend
Chatbot:

> Finished LLMChain chain.
' Hi there, how are you doing today?'
llm_chain.predict(human_input="Not too bad - how are you?")
> Entering new LLMChain chain...
Prompt after formatting:
You are a chatbot having a conversation with a human.


Human: Hi there my friend
AI:  Hi there, how are you doing today?
Human: Not to bad - how are you?
Chatbot:

> Finished LLMChain chain.
" I'm doing great, thank you for asking!"



How to add memory to a Multi-Input Chain
Most memory objects assume a single input. In this notebook, we go over how to add memory to a chain that has multiple inputs. As an example of such a chain, we will add memory to a question/answering chain. This chain takes as inputs both related documents and a user question.

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.cohere import CohereEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
with open('../../state_of_the_union.txt') as f:
    state_of_the_union = f.read()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_text(state_of_the_union)

embeddings = OpenAIEmbeddings()
docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{"source": i} for i in range(len(texts))])
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transient.
query = "What did the president say about Justice Breyer"
docs = docsearch.similarity_search(query)
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
template = """You are a chatbot having a conversation with a human.

Given the following extracted parts of a long document and a question, create a final answer.

{context}

{chat_history}
Human: {human_input}
Chatbot:"""

prompt = PromptTemplate(
    input_variables=["chat_history", "human_input", "context"], 
    template=template
)
memory = ConversationBufferMemory(memory_key="chat_history", input_key="human_input")
chain = load_qa_chain(OpenAI(temperature=0), chain_type="stuff", memory=memory, prompt=prompt)
query = "What did the president say about Justice Breyer"
chain({"input_documents": docs, "human_input": query}, return_only_outputs=True)
{'output_text': ' Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.'}
print(chain.memory.buffer)
Human: What did the president say about Justice Breyer
AI:  Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.



Cassandra Chat Message History
This notebook goes over how to use Cassandra to store chat message history.

Cassandra is a distributed database that is well suited for storing large amounts of data.

It is a good choice for storing chat message history because it is easy to scale and can handle a large number of writes.

# List of contact points to try connecting to Cassandra cluster.
contact_points = ["cassandra"]
from langchain.memory import CassandraChatMessageHistory

message_history = CassandraChatMessageHistory(
    contact_points=contact_points, session_id="test-session"
)

message_history.add_user_message("hi!")

message_history.add_ai_message("whats up?")
message_history.messages
[HumanMessage(content='hi!', additional_kwargs={}, example=False),
 AIMessage(content='whats up?', additional_kwargs={}, example=False)]


 Postgres Chat Message History
This notebook goes over how to use Postgres to store chat message history.

from langchain.memory import PostgresChatMessageHistory

history = PostgresChatMessageHistory(connection_string="postgresql://postgres:mypassword@localhost/chat_history", session_id="foo")

history.add_user_message("hi!")

history.add_ai_message("whats up?")
history.messages


Getting Started
In this tutorial, we will learn about creating simple chains in LangChain. We will learn how to create a chain, add components to it, and run it.

In this tutorial, we will cover:

Using a simple LLM chain

Creating sequential chains

Creating a custom chain

Why do we need chains?
Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.

Quick start: Using LLMChain
The LLMChain is a simple chain that takes in a prompt template, formats it with the user input and returns the response from an LLM.

To use the LLMChain, first create a prompt template.

from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

llm = OpenAI(temperature=0.9)
prompt = PromptTemplate(
    input_variables=["product"],
    template="What is a good name for a company that makes {product}?",
)
We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)

# Run the chain only specifying the input variable.
print(chain.run("colorful socks"))
SockSplash!
You can use a chat model in an LLMChain as well:

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)
human_message_prompt = HumanMessagePromptTemplate(
        prompt=PromptTemplate(
            template="What is a good name for a company that makes {product}?",
            input_variables=["product"],
        )
    )
chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])
chat = ChatOpenAI(temperature=0.9)
chain = LLMChain(llm=chat, prompt=chat_prompt_template)
print(chain.run("colorful socks"))
Rainbow Sox Co.
Different ways of calling chains
All classes inherited from Chain offer a few ways of running chain logic. The most direct one is by using __call__:

chat = ChatOpenAI(temperature=0)
prompt_template = "Tell me a {adjective} joke"
llm_chain = LLMChain(
    llm=chat,
    prompt=PromptTemplate.from_template(prompt_template)
)

llm_chain(inputs={"adjective":"corny"})
{'adjective': 'corny',
 'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}
By default, __call__ returns both the input and output key values. You can configure it to only return output key values by setting return_only_outputs to True.

llm_chain("corny", return_only_outputs=True)
{'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}
If the Chain only outputs one output key (i.e. only has one element in its output_keys), you can use run method. Note that run outputs a string instead of a dictionary.

# llm_chain only has one output key, so we can use run
llm_chain.output_keys
['text']
llm_chain.run({"adjective":"corny"})
'Why did the tomato turn red? Because it saw the salad dressing!'
In the case of one input key, you can input the string directly without specifying the input mapping.

# These two are equivalent
llm_chain.run({"adjective":"corny"})
llm_chain.run("corny")

# These two are also equivalent
llm_chain("corny")
llm_chain({"adjective":"corny"})
{'adjective': 'corny',
 'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}
Tips: You can easily integrate a Chain object as a Tool in your Agent via its run method. See an example here.

Add memory to chains
Chain supports taking a BaseMemory object as its memory argument, allowing Chain object to persist data across multiple calls. In other words, it makes Chain a stateful object.

from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

conversation = ConversationChain(
    llm=chat,
    memory=ConversationBufferMemory()
)

conversation.run("Answer briefly. What are the first 3 colors of a rainbow?")
# -> The first three colors of a rainbow are red, orange, and yellow.
conversation.run("And the next 4?")
# -> The next four colors of a rainbow are green, blue, indigo, and violet.
'The next four colors of a rainbow are green, blue, indigo, and violet.'
Essentially, BaseMemory defines an interface of how langchain stores memory. It allows reading of stored data through load_memory_variables method and storing new data through save_context method. You can learn more about it in Memory section.

Debug Chain
It can be hard to debug Chain object solely from its output as most Chain objects involve a fair amount of input prompt preprocessing and LLM output post-processing. Setting verbose to True will print out some internal states of the Chain object while it is being ran.

conversation = ConversationChain(
    llm=chat,
    memory=ConversationBufferMemory(),
    verbose=True
)
conversation.run("What is ChatGPT?")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: What is ChatGPT?
AI:

> Finished chain.
'ChatGPT is an AI language model developed by OpenAI. It is based on the GPT-3 architecture and is capable of generating human-like responses to text prompts. ChatGPT has been trained on a massive amount of text data and can understand and respond to a wide range of topics. It is often used for chatbots, virtual assistants, and other conversational AI applications.'
Combine chains with the SequentialChain
The next step after calling a language model is to make a series of calls to a language model. We can do this using sequential chains, which are chains that execute their links in a predefined order. Specifically, we will use the SimpleSequentialChain. This is the simplest type of a sequential chain, where each step has a single input/output, and the output of one step is the input to the next.

In this tutorial, our sequential chain will:

First, create a company name for a product. We will reuse the LLMChain we’d previously initialized to create this company name.

Then, create a catchphrase for the product. We will initialize a new LLMChain to create this catchphrase, as shown below.

second_prompt = PromptTemplate(
    input_variables=["company_name"],
    template="Write a catchphrase for the following company: {company_name}",
)
chain_two = LLMChain(llm=llm, prompt=second_prompt)
Now we can combine the two LLMChains, so that we can create a company name and a catchphrase in a single step.

from langchain.chains import SimpleSequentialChain
overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)

# Run the chain specifying only the input variable for the first chain.
catchphrase = overall_chain.run("colorful socks")
print(catchphrase)
> Entering new SimpleSequentialChain chain...
Rainbow Socks Co.


"Step into Color with Rainbow Socks!"

> Finished chain.


"Step into Color with Rainbow Socks!"
Create a custom chain with the Chain class
LangChain provides many chains out of the box, but sometimes you may want to create a custom chain for your specific use case. For this example, we will create a custom chain that concatenates the outputs of 2 LLMChains.

In order to create a custom chain:

Start by subclassing the Chain class,

Fill out the input_keys and output_keys properties,

Add the _call method that shows how to execute the chain.

These steps are demonstrated in the example below:

from langchain.chains import LLMChain
from langchain.chains.base import Chain

from typing import Dict, List


class ConcatenateChain(Chain):
    chain_1: LLMChain
    chain_2: LLMChain

    @property
    def input_keys(self) -> List[str]:
        # Union of the input keys of the two chains.
        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))
        return list(all_input_vars)

    @property
    def output_keys(self) -> List[str]:
        return ['concat_output']

    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:
        output_1 = self.chain_1.run(inputs)
        output_2 = self.chain_2.run(inputs)
        return {'concat_output': output_1 + output_2}
Now, we can try running the chain that we called.

prompt_1 = PromptTemplate(
    input_variables=["product"],
    template="What is a good name for a company that makes {product}?",
)
chain_1 = LLMChain(llm=llm, prompt=prompt_1)

prompt_2 = PromptTemplate(
    input_variables=["product"],
    template="What is a good slogan for a company that makes {product}?",
)
chain_2 = LLMChain(llm=llm, prompt=prompt_2)

concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)
concat_output = concat_chain.run("colorful socks")
print(f"Concatenated output:\n{concat_output}")
Concatenated output:


Socktastic Colors.

"Put Some Color in Your Step!"







Callbacks
LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.

You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail. There are two main callbacks mechanisms:

Constructor callbacks will be used for all calls made on that object, and will be scoped to that object only, i.e. if you pass a handler to the LLMChain constructor, it will not be used by the model attached to that chain.

Request callbacks will be used for that specific request only, and all sub-requests that it contains (eg. a call to an LLMChain triggers a call to a Model, which uses the same handler passed through). These are explicitly passed through.

Advanced: When you create a custom chain you can easily set it up to use the same callback system as all the built-in chains. _call, _generate, _run, and equivalent async methods on Chains / LLMs / Chat Models / Agents / Tools now receive a 2nd argument called run_manager which is bound to that run, and contains the logging methods that can be used by that object (i.e. on_llm_new_token). This is useful when constructing a custom chain. See this guide for more information on how to create custom chains and use callbacks inside them.

CallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered.

class BaseCallbackHandler:
    """Base callback handler that can be used to handle callbacks from langchain."""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Run when LLM starts running."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """Run on new LLM token. Only available when streaming is enabled."""

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """Run when LLM ends running."""

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when LLM errors."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """Run when chain starts running."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """Run when chain ends running."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when chain errors."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """Run when tool starts running."""

    def on_tool_end(self, output: str, **kwargs: Any) -> Any:
        """Run when tool ends running."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when tool errors."""

    def on_text(self, text: str, **kwargs: Any) -> Any:
        """Run on arbitrary text."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Run on agent action."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        """Run on agent end."""
How to use callbacks
The callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places:

Constructor callbacks: defined in the constructor, eg. LLMChain(callbacks=[handler]), which will be used for all calls made on that object, and will be scoped to that object only, eg. if you pass a handler to the LLMChain constructor, it will not be used by the Model attached to that chain.

Request callbacks: defined in the call()/run()/apply() methods used for issuing a request, eg. chain.call(inputs, callbacks=[handler]), which will be used for that specific request only, and all sub-requests that it contains (eg. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the call() method).

The verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, eg. LLMChain(verbose=True), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. This is useful for debugging, as it will log all events to the console.

When do you want to use each of these?
Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are not specific to a single request, but rather to the entire chain. For example, if you want to log all the requests made to an LLMChain, you would pass a handler to the constructor.

Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the call() method

Using an existing handler
LangChain provides a few built-in handlers that you can use to get started. These are available in the langchain/callbacks module. The most basic handler is the StdOutCallbackHandler, which simply logs all events to stdout. In the future we will add more default handlers to the library.

Note when the verbose flag on the object is set to true, the StdOutCallbackHandler will be invoked even without being explicitly passed in.

from langchain.callbacks import StdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

# First, let's explicitly set the StdOutCallbackHandler in `callbacks`
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])
chain.run(number=2)

# Then, let's use the `verbose` flag to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt, verbose=True)
chain.run(number=2)

# Finally, let's use the request `callbacks` to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt)
chain.run(number=2, callbacks=[handler])
> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 = 

> Finished chain.


> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 = 

> Finished chain.


> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 = 

> Finished chain.
'\n\n3'
Creating a custom handler
You can create a custom handler to set on the object as well. In the example below, we’ll implement streaming with a custom handler.

from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

class MyCustomHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"My custom handler, token: {token}")

# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in a list with our custom handler
chat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomHandler()])

chat([HumanMessage(content="Tell me a joke")])
My custom handler, token: 
My custom handler, token: Why
My custom handler, token:  did
My custom handler, token:  the
My custom handler, token:  tomato
My custom handler, token:  turn
My custom handler, token:  red
My custom handler, token: ?
My custom handler, token:  Because
My custom handler, token:  it
My custom handler, token:  saw
My custom handler, token:  the
My custom handler, token:  salad
My custom handler, token:  dressing
My custom handler, token: !
My custom handler, token: 
AIMessage(content='Why did the tomato turn red? Because it saw the salad dressing!', additional_kwargs={})
Async Callbacks
If you are planning to use the async API, it is recommended to use AsyncCallbackHandler to avoid blocking the runloop.

Advanced if you use a sync CallbackHandler while using an async method to run your llm/chain/tool/agent, it will still work. However, under the hood, it will be called with run_in_executor which can cause issues if your CallbackHandler is not thread-safe.

import asyncio
from typing import Any, Dict, List
from langchain.schema import LLMResult
from langchain.callbacks.base import AsyncCallbackHandler

class MyCustomSyncHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"Sync handler being called in a `thread_pool_executor`: token: {token}")

class MyCustomAsyncHandler(AsyncCallbackHandler):
    """Async callback handler that can be used to handle callbacks from langchain."""

    async def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when chain starts running."""
        print("zzzz....")
        await asyncio.sleep(0.3)
        class_name = serialized["name"]
        print("Hi! I just woke up. Your llm is starting")

    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when chain ends running."""
        print("zzzz....")
        await asyncio.sleep(0.3)
        print("Hi! I just woke up. Your llm is ending")

# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in a list with our custom handler
chat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()])

await chat.agenerate([[HumanMessage(content="Tell me a joke")]])
zzzz....
Hi! I just woke up. Your llm is starting
Sync handler being called in a `thread_pool_executor`: token: 
Sync handler being called in a `thread_pool_executor`: token: Why
Sync handler being called in a `thread_pool_executor`: token:  don
Sync handler being called in a `thread_pool_executor`: token: 't
Sync handler being called in a `thread_pool_executor`: token:  scientists
Sync handler being called in a `thread_pool_executor`: token:  trust
Sync handler being called in a `thread_pool_executor`: token:  atoms
Sync handler being called in a `thread_pool_executor`: token: ?


Sync handler being called in a `thread_pool_executor`: token: Because
Sync handler being called in a `thread_pool_executor`: token:  they
Sync handler being called in a `thread_pool_executor`: token:  make
Sync handler being called in a `thread_pool_executor`: token:  up
Sync handler being called in a `thread_pool_executor`: token:  everything
Sync handler being called in a `thread_pool_executor`: token: !
Sync handler being called in a `thread_pool_executor`: token: 
zzzz....
Hi! I just woke up. Your llm is ending
LLMResult(generations=[[ChatGeneration(text="Why don't scientists trust atoms?\n\nBecause they make up everything!", generation_info=None, message=AIMessage(content="Why don't scientists trust atoms?\n\nBecause they make up everything!", additional_kwargs={}))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'})
Using multiple handlers, passing in handlers
In the previous examples, we passed in callback handlers upon creation of an object by using callbacks=. In this case, the callbacks will be scoped to that particular object.

However, in many cases, it is advantageous to pass in handlers instead when running the object. When we pass through CallbackHandlers using the callbacks keyword arg when executing an run, those callbacks will be issued by all nested objects involved in the execution. For example, when a handler is passed through to an Agent, it will be used for all callbacks related to the agent and all the objects involved in the agent’s execution, in this case, the Tools, LLMChain, and LLM.

This prevents us from having to manually attach the handlers to each individual nested object.

from typing import Dict, Union, Any, List

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import AgentAction
from langchain.agents import AgentType, initialize_agent, load_tools
from langchain.callbacks import tracing_enabled
from langchain.llms import OpenAI

# First, define custom callback handler implementations
class MyCustomHandlerOne(BaseCallbackHandler):
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        print(f"on_llm_start {serialized['name']}")

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        print(f"on_new_token {token}")

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when LLM errors."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        print(f"on_chain_start {serialized['name']}")

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        print(f"on_tool_start {serialized['name']}")

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        print(f"on_agent_action {action}")

class MyCustomHandlerTwo(BaseCallbackHandler):
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        print(f"on_llm_start (I'm the second handler!!) {serialized['name']}")

# Instantiate the handlers
handler1 = MyCustomHandlerOne()
handler2 = MyCustomHandlerTwo()

# Setup the agent. Only the `llm` will issue callbacks for handler2
llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])
tools = load_tools(["llm-math"], llm=llm)
agent = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION
)

# Callbacks for handler1 will be issued by every object involved in the 
# Agent execution (llm, llmchain, tool, agent executor)
agent.run("What is 2 raised to the 0.235 power?", callbacks=[handler1])
on_chain_start AgentExecutor
on_chain_start LLMChain
on_llm_start OpenAI
on_llm_start (I'm the second handler!!) OpenAI
on_new_token  I
on_new_token  need
on_new_token  to
on_new_token  use
on_new_token  a
on_new_token  calculator
on_new_token  to
on_new_token  solve
on_new_token  this
on_new_token .
on_new_token 
Action
on_new_token :
on_new_token  Calculator
on_new_token 
Action
on_new_token  Input
on_new_token :
on_new_token  2
on_new_token ^
on_new_token 0
on_new_token .
on_new_token 235
on_new_token 
on_agent_action AgentAction(tool='Calculator', tool_input='2^0.235', log=' I need to use a calculator to solve this.\nAction: Calculator\nAction Input: 2^0.235')
on_tool_start Calculator
on_chain_start LLMMathChain
on_chain_start LLMChain
on_llm_start OpenAI
on_llm_start (I'm the second handler!!) OpenAI
on_new_token 

on_new_token ```text
on_new_token 

on_new_token 2
on_new_token **
on_new_token 0
on_new_token .
on_new_token 235
on_new_token 

on_new_token ```

on_new_token ...
on_new_token num
on_new_token expr
on_new_token .
on_new_token evaluate
on_new_token ("
on_new_token 2
on_new_token **
on_new_token 0
on_new_token .
on_new_token 235
on_new_token ")
on_new_token ...
on_new_token 

on_new_token 
on_chain_start LLMChain
on_llm_start OpenAI
on_llm_start (I'm the second handler!!) OpenAI
on_new_token  I
on_new_token  now
on_new_token  know
on_new_token  the
on_new_token  final
on_new_token  answer
on_new_token .
on_new_token 
Final
on_new_token  Answer
on_new_token :
on_new_token  1
on_new_token .
on_new_token 17
on_new_token 690
on_new_token 67
on_new_token 372
on_new_token 187
on_new_token 674
on_new_token 
'1.1769067372187674'
Tracing and Token Counting
Tracing and token counting are two capabilities we provide which are built on our callbacks mechanism.

Tracing
There are two recommended ways to trace your LangChains:

Setting the LANGCHAIN_TRACING environment variable to "true".

Using a context manager with tracing_enabled() to trace a particular block of code.

Note if the environment variable is set, all code will be traced, regardless of whether or not it’s within the context manager.

import os

from langchain.agents import AgentType, initialize_agent, load_tools
from langchain.callbacks import tracing_enabled
from langchain.llms import OpenAI

# To run the code, make sure to set OPENAI_API_KEY and SERPAPI_API_KEY
llm = OpenAI(temperature=0)
tools = load_tools(["llm-math", "serpapi"], llm=llm)
agent = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

questions = [
    "Who won the US Open men's final in 2019? What is his age raised to the 0.334 power?",
    "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
    "Who won the most recent formula 1 grand prix? What is their age raised to the 0.23 power?",
    "Who won the US Open women's final in 2019? What is her age raised to the 0.34 power?",
    "Who is Beyonce's husband? What is his age raised to the 0.19 power?",
]
os.environ["LANGCHAIN_TRACING"] = "true"

# Both of the agent runs will be traced because the environment variable is set
agent.run(questions[0])
with tracing_enabled() as session:
    assert session
    agent.run(questions[1])
> Entering new AgentExecutor chain...
 I need to find out who won the US Open men's final in 2019 and then calculate his age raised to the 0.334 power.
Action: Search
Action Input: "US Open men's final 2019 winner"
Observation: Rafael Nadal defeated Daniil Medvedev in the final, 7–5, 6–3, 5–7, 4–6, 6–4 to win the men's singles tennis title at the 2019 US Open. It was his fourth US ...
Thought: I need to find out the age of the winner
Action: Search
Action Input: "Rafael Nadal age"
Observation: 36 years
Thought: I need to calculate the age raised to the 0.334 power
Action: Calculator
Action Input: 36^0.334
Observation: Answer: 3.3098250249682484
Thought: I now know the final answer
Final Answer: Rafael Nadal, aged 36, won the US Open men's final in 2019 and his age raised to the 0.334 power is 3.3098250249682484.

> Finished chain.


> Entering new AgentExecutor chain...
 I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
Action: Search
Action Input: "Olivia Wilde boyfriend"
Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.
Thought: I need to find out Harry Styles' age.
Action: Search
Action Input: "Harry Styles age"
Observation: 29 years
Thought: I need to calculate 29 raised to the 0.23 power.
Action: Calculator
Action Input: 29^0.23
Observation: Answer: 2.169459462491557
Thought: I now know the final answer.
Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.

> Finished chain.
# Now, we unset the environment variable and use a context manager.

if "LANGCHAIN_TRACING" in os.environ:
    del os.environ["LANGCHAIN_TRACING"]

# here, we are writing traces to "my_test_session"
with tracing_enabled("my_test_session") as session:
    assert session
    agent.run(questions[0])  # this should be traced

agent.run(questions[1])  # this should not be traced
> Entering new AgentExecutor chain...
 I need to find out who won the US Open men's final in 2019 and then calculate his age raised to the 0.334 power.
Action: Search
Action Input: "US Open men's final 2019 winner"
Observation: Rafael Nadal defeated Daniil Medvedev in the final, 7–5, 6–3, 5–7, 4–6, 6–4 to win the men's singles tennis title at the 2019 US Open. It was his fourth US ...
Thought: I need to find out the age of the winner
Action: Search
Action Input: "Rafael Nadal age"
Observation: 36 years
Thought: I need to calculate the age raised to the 0.334 power
Action: Calculator
Action Input: 36^0.334
Observation: Answer: 3.3098250249682484
Thought: I now know the final answer
Final Answer: Rafael Nadal, aged 36, won the US Open men's final in 2019 and his age raised to the 0.334 power is 3.3098250249682484.

> Finished chain.


> Entering new AgentExecutor chain...
 I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
Action: Search
Action Input: "Olivia Wilde boyfriend"
Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.
Thought: I need to find out Harry Styles' age.
Action: Search
Action Input: "Harry Styles age"
Observation: 29 years
Thought: I need to calculate 29 raised to the 0.23 power.
Action: Calculator
Action Input: 29^0.23
Observation: Answer: 2.169459462491557
Thought: I now know the final answer.
Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.

> Finished chain.
"Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
# The context manager is concurrency safe:
if "LANGCHAIN_TRACING" in os.environ:
    del os.environ["LANGCHAIN_TRACING"]

# start a background task
task = asyncio.create_task(agent.arun(questions[0]))  # this should not be traced
with tracing_enabled() as session:
    assert session
    tasks = [agent.arun(q) for q in questions[1:3]]  # these should be traced
    await asyncio.gather(*tasks)

await task
> Entering new AgentExecutor chain...

> Entering new AgentExecutor chain...


> Entering new AgentExecutor chain...

 I need to find out who won the grand prix and then calculate their age raised to the 0.23 power.
Action: Search
Action Input: "Formula 1 Grand Prix Winner" I need to find out who won the US Open men's final in 2019 and then calculate his age raised to the 0.334 power.
Action: Search
Action Input: "US Open men's final 2019 winner"Rafael Nadal defeated Daniil Medvedev in the final, 7–5, 6–3, 5–7, 4–6, 6–4 to win the men's singles tennis title at the 2019 US Open. It was his fourth US ... I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
Action: Search
Action Input: "Olivia Wilde boyfriend"Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.Lewis Hamilton has won 103 Grands Prix during his career. He won 21 races with McLaren and has won 82 with Mercedes. Lewis Hamilton holds the record for the ... I need to find out the age of the winner
Action: Search
Action Input: "Rafael Nadal age"36 years I need to find out Harry Styles' age.
Action: Search
Action Input: "Harry Styles age" I need to find out Lewis Hamilton's age
Action: Search
Action Input: "Lewis Hamilton Age"29 years I need to calculate the age raised to the 0.334 power
Action: Calculator
Action Input: 36^0.334 I need to calculate 29 raised to the 0.23 power.
Action: Calculator
Action Input: 29^0.23Answer: 3.3098250249682484Answer: 2.16945946249155738 years
> Finished chain.

> Finished chain.
 I now need to calculate 38 raised to the 0.23 power
Action: Calculator
Action Input: 38^0.23Answer: 2.3086081644669734
> Finished chain.
"Rafael Nadal, aged 36, won the US Open men's final in 2019 and his age raised to the 0.334 power is 3.3098250249682484."
Token Counting
LangChain offers a context manager that allows you to count tokens.

from langchain.callbacks import get_openai_callback

llm = OpenAI(temperature=0)
with get_openai_callback() as cb:
    llm("What is the square root of 4?")

total_tokens = cb.total_tokens
assert total_tokens > 0

with get_openai_callback() as cb:
    llm("What is the square root of 4?")
    llm("What is the square root of 4?")

assert cb.total_tokens == total_tokens * 2

# You can kick off concurrent runs from within the context manager
with get_openai_callback() as cb:
    await asyncio.gather(
        *[llm.agenerate(["What is the square root of 4?"]) for _ in range(3)]
    )

assert cb.total_tokens == total_tokens * 3

# The context manager is concurrency safe
task = asyncio.create_task(llm.agenerate(["What is the square root of 4?"]))
with get_openai_callback() as cb:
    await llm.agenerate(["What is the square root of 4?"])

await task
assert cb.total_tokens == total_tokens